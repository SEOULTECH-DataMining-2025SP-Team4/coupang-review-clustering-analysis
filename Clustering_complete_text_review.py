#%%
import pandas as pd
#%%
df = pd.read_csv('data/Coupang-reviews-homeplanet/review_data_complete_text.csv')
df.head()
#%%
# í—¤ë“œë¼ì¸ê³¼ ë¦¬ë·°ë‚´ìš©ì˜ ì¡´ì¬ ì—¬ë¶€ë¥¼ ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ í‘œí˜„
df['í—¤ë“œë¼ì¸_ì¡´ì¬'] = df['í—¤ë“œë¼ì¸'].notna().astype(int)
df['ë¦¬ë·°ë‚´ìš©_ì¡´ì¬'] = df['ë¦¬ë·°ë‚´ìš©'].notna().astype(int)

# ê²°ê³¼ í™•ì¸
print("ì›-í•« ì¸ì½”ë”© ê²°ê³¼:")
print(df[['í—¤ë“œë¼ì¸', 'ë¦¬ë·°ë‚´ìš©', 'í—¤ë“œë¼ì¸_ì¡´ì¬', 'ë¦¬ë·°ë‚´ìš©_ì¡´ì¬']].head())

# ê° íŒ¨í„´ ë¶„í¬ í™•ì¸
pattern_counts = df.groupby(['í—¤ë“œë¼ì¸_ì¡´ì¬', 'ë¦¬ë·°ë‚´ìš©_ì¡´ì¬']).size().reset_index(name='ê°œìˆ˜')
pattern_counts['íŒ¨í„´'] = pattern_counts.apply(
    lambda x: f"í—¤ë“œë¼ì¸{'O' if x['í—¤ë“œë¼ì¸_ì¡´ì¬']==1 else 'X'}-ë¦¬ë·°ë‚´ìš©{'O' if x['ë¦¬ë·°ë‚´ìš©_ì¡´ì¬']==1 else 'X'}",
    axis=1
)
print("\níŒ¨í„´ë³„ ë¶„í¬:")
print(pattern_counts[['íŒ¨í„´', 'ê°œìˆ˜']])
#%%
# í—¤ë“œë¼ì¸ê³¼ ë¦¬ë·°ë‚´ìš©ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ëŠ” ì‘ì—…
df['full_text'] = df['í—¤ë“œë¼ì¸'].fillna('') + ' ' + df['ë¦¬ë·°ë‚´ìš©'].fillna('')
#%%
# í…ìŠ¤íŠ¸ì˜ ê¸¸ì´ë¥¼ featureë¡œ ì¶”ì¶œ
df['full_text_length'] = df['full_text'].str.len()
#%%
!pip install emoji --quiet
#%%
import emoji

# ì´ëª¨í‹°ì½˜ ê°œìˆ˜ë¥¼ ì„¸ëŠ” í•¨ìˆ˜ > í•˜ë‚˜ì˜ featureë¡œ ì¶”ì¶œ
def count_emojis(text):
    if not isinstance(text, str):
        return 0
    return sum(1 for c in text if c in emoji.EMOJI_DATA)

# ì´ëª¨í‹°ì½˜ ê°œìˆ˜ ì»¬ëŸ¼ ì¶”ê°€
df['emoticon_count'] = df['full_text'].apply(count_emojis)

# ê²°ê³¼ í™•ì¸
df[['full_text', 'emoticon_count']].head(10)
#%%
# ê°ì •ë¶„ì„ ëª¨ë“ˆì…ë‹ˆë‹¤
# êµ°ì‚°ëŒ€í•™êµ KNU í•œêµ­ì–´ ê°ì •ë‹¨ì–´ ë§ë­‰ì¹˜ ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ë¶„ì„

import pandas as pd
import numpy as np
import re
import json
from collections import defaultdict

class KnuSentiLexAnalyzer:
    def __init__(self, json_data=None):
        if json_data:
            self.sentiment_dict = self._parse_knusentilex_json(json_data)
        else:
            self.sentiment_dict = self._create_sample_sentiment_dict()

        # ê°ì • ë¶„ë¥˜ë¥¼ ìœ„í•œ ì„ê³„ê°’ ì„¤ì •
        # ì‹¤ì œ KnuSentiLex ë°ì´í„°ëŠ” -2~2 ë²”ìœ„ì´ë¯€ë¡œ ì„ê³„ê°’ì„ ì¡°ì •í•©ë‹ˆë‹¤ / í•˜ë‚˜ì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¡œ ê²°ê³¼ë¥¼ ë³´ê³  íŠœë‹
        self.positive_threshold = 0.5
        self.negative_threshold = -0.5

        print(f"KnuSentiLex ê°ì •ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ê°ì •ì‚¬ì „ í¬ê¸°: {len(self.sentiment_dict):,}ê°œ ë‹¨ì–´")
        print(f"   ê¸ì • ì„ê³„ê°’: {self.positive_threshold}")
        print(f"   ë¶€ì • ì„ê³„ê°’: {self.negative_threshold}")

    def _parse_knusentilex_json(self, json_data):

        sentiment_dict = {}

        for item in json_data:
            try:
                word = item.get('word', '').strip()
                polarity = item.get('polarity', '0')
                word_root = item.get('word_root', '').strip()

                # polarityë¥¼ ìˆ«ìë¡œ ë³€í™˜
                try:
                    score = float(polarity)
                except (ValueError, TypeError):
                    score = 0.0

                # ë¹ˆ ë‹¨ì–´ëŠ” ì œì™¸
                if word and len(word.strip()) > 0:
                    sentiment_dict[word] = {
                        'score': score,
                        'word_root': word_root,
                        'original_polarity': polarity
                    }

                    # ì–´ê·¼ ì •ë³´ë„ ë³„ë„ë¡œ ì €ì¥ (ì–´ê·¼ìœ¼ë¡œë„ ê²€ìƒ‰ ê°€ëŠ¥í•˜ë„ë¡)
                    # ì´ë ‡ê²Œ í•˜ë©´ "ê°€ë‚œí•˜ë‹¤", "ê°€ë‚œí•´ìš”" ë“±ì´ ëª¨ë‘ "ê°€ë‚œ" ì–´ê·¼ìœ¼ë¡œ ë§¤ì¹­ë©ë‹ˆë‹¤
                    if word_root and len(word_root.strip()) > 1 and word_root != word:
                        # ì–´ê·¼ì€ ê¸°ì¡´ ë‹¨ì–´ë³´ë‹¤ ì•½ê°„ ë‚®ì€ ê°€ì¤‘ì¹˜ë¡œ ì €ì¥
                        if word_root not in sentiment_dict:
                            sentiment_dict[word_root] = {
                                'score': score * 0.8,  # ì–´ê·¼ì€ 80% ê°€ì¤‘ì¹˜
                                'word_root': word_root,
                                'original_polarity': polarity,
                                'is_root': True
                            }

            except Exception as e:
                # ê°œë³„ í•­ëª© íŒŒì‹± ì‹¤íŒ¨ì‹œ ê±´ë„ˆë›°ê¸°
                continue

        print(f"íŒŒì‹± ì™„ë£Œ: {len(sentiment_dict):,}ê°œ ê°ì • ë‹¨ì–´ ë¡œë“œ")

        # ê°ì •ë³„ ë¶„í¬ í™•ì¸
        positive_count = sum(1 for item in sentiment_dict.values() if item['score'] > 0)
        negative_count = sum(1 for item in sentiment_dict.values() if item['score'] < 0)
        neutral_count = sum(1 for item in sentiment_dict.values() if item['score'] == 0)

        print(f"   ğŸ“ˆ ê°ì • ë¶„í¬: ê¸ì • {positive_count:,}ê°œ, ë¶€ì • {negative_count:,}ê°œ, ì¤‘ë¦½ {neutral_count:,}ê°œ")

        return sentiment_dict

    def _create_sample_sentiment_dict(self):
        print("âš ï¸  ì‹¤ì œ KnuSentiLex ë°ì´í„°ê°€ ì œê³µë˜ì§€ ì•Šì•„ ìƒ˜í”Œ ì‚¬ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤")
        print("   ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ ì‹¤ì œ JSON ë°ì´í„° ì œê³µì„ ê¶Œì¥í•©ë‹ˆë‹¤")

        # ê¸°ë³¸ ìƒ˜í”Œ ê°ì •ì‚¬ì „ (ì‹¤ì œ KnuSentiLex í˜•íƒœë¡œ êµ¬ì„±)
        sample_data = {
            # ê¸ì • ë‹¨ì–´ë“¤
            'ì¢‹ë‹¤': {'score': 2.0, 'word_root': 'ì¢‹', 'original_polarity': '2'},
            'í›Œë¥­í•˜ë‹¤': {'score': 2.0, 'word_root': 'í›Œë¥­', 'original_polarity': '2'},
            'ë§Œì¡±í•˜ë‹¤': {'score': 2.0, 'word_root': 'ë§Œì¡±', 'original_polarity': '2'},
            'ì¶”ì²œí•˜ë‹¤': {'score': 1.0, 'word_root': 'ì¶”ì²œ', 'original_polarity': '1'},
            'ìµœê³ ': {'score': 2.0, 'word_root': 'ìµœê³ ', 'original_polarity': '2'},
            '^_^': {'score': 1.0, 'word_root': '^_^', 'original_polarity': '1'},
            '^^': {'score': 1.0, 'word_root': '^^', 'original_polarity': '1'},

            # ë¶€ì • ë‹¨ì–´ë“¤
            'ë‚˜ì˜ë‹¤': {'score': -2.0, 'word_root': 'ë‚˜ì˜', 'original_polarity': '-2'},
            'ìµœì•…': {'score': -2.0, 'word_root': 'ìµœì•…', 'original_polarity': '-2'},
            'ì‹¤ë§í•˜ë‹¤': {'score': -2.0, 'word_root': 'ì‹¤ë§', 'original_polarity': '-2'},
            'ë¬¸ì œ': {'score': -1.0, 'word_root': 'ë¬¸ì œ', 'original_polarity': '-1'},
            ':-|': {'score': -1.0, 'word_root': ':', 'original_polarity': '-1'},

            # ì¤‘ë¦½ ë‹¨ì–´ë“¤
            'ë³´í†µ': {'score': 0.0, 'word_root': 'ë³´í†µ', 'original_polarity': '0'},
            'ì¼ë°˜ì ': {'score': 0.0, 'word_root': 'ì¼ë°˜', 'original_polarity': '0'}
        }

        return sample_data

    def _preprocess_text(self, text):
        """
        í…ìŠ¤íŠ¸ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤

        KnuSentiLex ë°ì´í„°ì—ëŠ” ì´ëª¨í‹°ì½˜ë„ í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ,
        ì´ëª¨í‹°ì½˜ì„ ë³´ì¡´í•˜ë©´ì„œ ë‹¤ë¥¸ ë¶ˆí•„ìš”í•œ íŠ¹ìˆ˜ë¬¸ìë§Œ ì œê±°í•˜ëŠ”
        ë” ì •êµí•œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        if not text or pd.isna(text):
            return ""

        text = str(text)

        # ì´ëª¨í‹°ì½˜ íŒ¨í„´ ë³´ì¡´ì„ ìœ„í•´ ì„ì‹œë¡œ íŠ¹ë³„í•œ í† í°ìœ¼ë¡œ ë³€í™˜
        emoticon_patterns = [
            r'\([\^\-_oOTã…¡;]+\)',  # (^^), (-_-), (;_;) ë“±
            r'[:\;\=][D\)\(\|\\/\-pP]',  # :), :D, :-), =) ë“±
            r'XD', r'<3', r'\^\^', r'\*\^\^\*'  # XD, <3, ^^, *^^* ë“±
        ]

        # ì´ëª¨í‹°ì½˜ì„ ì„ì‹œ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë³´ì¡´
        preserved_emoticons = {}
        token_counter = 0

        for pattern in emoticon_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                emoticon = match.group()
                token = f"__EMOTICON_{token_counter}__"
                preserved_emoticons[token] = emoticon
                text = text.replace(emoticon, token, 1)
                token_counter += 1

        # ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ ì •ì œ
        text = re.sub(r'[^\w\sê°€-í£ã„±-ã…ã…-ã…£.,!?_]', ' ', text)  # _ëŠ” ì„ì‹œ í† í° ë•Œë¬¸ì— ë³´ì¡´
        text = re.sub(r'(.)\1{3,}', r'\1\1', text)  # ê³¼ë„í•œ ë°˜ë³µ ë¬¸ì ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)  # ë‹¤ì¤‘ ê³µë°± ì •ë¦¬

        # ì´ëª¨í‹°ì½˜ ë³µì›
        for token, emoticon in preserved_emoticons.items():
            text = text.replace(token, emoticon)

        return text.strip()

    def _calculate_sentiment(self, text):
        """
        í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ê³„ì‚°í•©ë‹ˆë‹¤

        ì´ í•¨ìˆ˜ëŠ” KnuSentiLexì˜ í’ë¶€í•œ ê°ì • ì •ë³´ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬
        ë” ì •í™•í•œ ê°ì • ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ë‹¨ìˆœí•œ ë‹¨ì–´ ë§¤ì¹­ë¿ë§Œ ì•„ë‹ˆë¼
        ì–´ê·¼ ì •ë³´ì™€ ê°ì • ê°•ë„ê¹Œì§€ ê³ ë ¤í•©ë‹ˆë‹¤.
        """
        if not text:
            return 'neutral'

        # ê³µë°±ê³¼ ê°„ë‹¨í•œ êµ¬ë‘ì ìœ¼ë¡œ ë‹¨ì–´ ë¶„í• 
        # ì´ëª¨í‹°ì½˜ë„ í•˜ë‚˜ì˜ "ë‹¨ì–´"ë¡œ ì·¨ê¸‰ë©ë‹ˆë‹¤
        words = re.findall(r'[^\s,.\!?]+', text)

        total_score = 0.0
        matched_count = 0

        for word in words:
            word = word.strip()
            if not word:
                continue

            # 1ë‹¨ê³„: ì™„ì „ ì¼ì¹˜ ê²€ìƒ‰
            if word in self.sentiment_dict:
                score = self.sentiment_dict[word]['score']
                total_score += score
                matched_count += 1
                continue

            # 2ë‹¨ê³„: ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ëŠ” ê²€ìƒ‰ (ì˜ì–´ ì´ëª¨í‹°ì½˜ ëŒ€ì‘)
            word_lower = word.lower()
            word_upper = word.upper()

            for candidate in [word_lower, word_upper]:
                if candidate in self.sentiment_dict:
                    score = self.sentiment_dict[candidate]['score']
                    total_score += score
                    matched_count += 1
                    break
            else:
                # 3ë‹¨ê³„: ë¶€ë¶„ ì¼ì¹˜ ê²€ìƒ‰ (ì–´ë¯¸ ë³€í™” ë“± ê³ ë ¤)
                # "ê°€ë‚œí•˜ë‹¤"ê°€ ì‚¬ì „ì— ìˆìœ¼ë©´ "ê°€ë‚œí•´ìš”", "ê°€ë‚œí–ˆì–´ìš”" ë“±ë„ ë§¤ì¹­
                best_match_score = 0
                best_match_length = 0

                for dict_word, word_info in self.sentiment_dict.items():
                    # ì‚¬ì „ ë‹¨ì–´ê°€ í˜„ì¬ ë‹¨ì–´ì— í¬í•¨ë˜ì–´ ìˆê³ , ì¶©ë¶„íˆ ê¸´ ê²½ìš°
                    if (len(dict_word) >= 2 and
                        dict_word in word and
                        len(dict_word) > best_match_length):

                        best_match_score = word_info['score'] * 0.7  # ë¶€ë¶„ì¼ì¹˜ëŠ” 70% ê°€ì¤‘ì¹˜
                        best_match_length = len(dict_word)

                if best_match_length > 0:
                    total_score += best_match_score
                    matched_count += 1

        # í‰ê·  ê°ì • ì ìˆ˜ ê³„ì‚°
        if matched_count > 0:
            avg_score = total_score / matched_count
        else:
            avg_score = 0.0

        # ê°ì • ë¶„ë¥˜ (ì„ê³„ê°’ ê¸°ë°˜)
        if avg_score >= self.positive_threshold:
            return 'positive'
        elif avg_score <= self.negative_threshold:
            return 'negative'
        else:
            return 'neutral'

    def analyze_sentiment(self, text):
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„í•©ë‹ˆë‹¤

        ì „ì²˜ë¦¬ë¶€í„° ìµœì¢… ë¶„ë¥˜ê¹Œì§€ì˜ ì „ì²´ ê³¼ì •ì„ ìˆ˜í–‰í•˜ì—¬
        ê°ì • ë¶„ë¥˜ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        preprocessed_text = self._preprocess_text(text)
        sentiment = self._calculate_sentiment(preprocessed_text)
        return sentiment

    def add_sentiment_columns(self, df, text_column='full_text'):
        """
        ë°ì´í„°í”„ë ˆì„ì— ê°ì •ë¶„ì„ ê²°ê³¼ë¥¼ ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤

        ì´ í•¨ìˆ˜ëŠ” ì‹¤ì œ KnuSentiLex ë°ì´í„°ì˜ ì¥ì ì„ ìµœëŒ€í•œ í™œìš©í•˜ì—¬
        ë” ì •í™•í•œ ê°ì •ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤. íŠ¹íˆ ì´ëª¨í‹°ì½˜ê³¼
        ë‹¤ì–‘í•œ ì–´ë¯¸ ë³€í™”ë¥¼ ì˜ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

        Args:
            df: ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„
            text_column: ë¶„ì„í•  í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…

        Returns:
            ê°ì •ë¶„ì„ ê²°ê³¼ê°€ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
        """
        result_df = df.copy()

        print(f"{len(df):,}ê°œ í…ìŠ¤íŠ¸ì— ëŒ€í•´ KnuSentiLex ê°ì •ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")

        # ê°ì •ë¶„ì„ ìˆ˜í–‰ (ì§„í–‰ë¥  í‘œì‹œ)
        sentiments = []
        for i, text in enumerate(result_df[text_column]):
            sentiment = self.analyze_sentiment(text)
            sentiments.append(sentiment)

            # ì§„í–‰ë¥  í‘œì‹œ (1000ê°œë§ˆë‹¤)
            if (i + 1) % 1000 == 0 or i == len(result_df) - 1:
                progress = (i + 1) / len(result_df) * 100
                print(f"   ì§„í–‰ë¥ : {i + 1:,}/{len(result_df):,} ({progress:.1f}%)")

        # ì›-í•« ì¸ì½”ë”© ìƒì„±
        result_df['sentiment_positive'] = (pd.Series(sentiments) == 'positive').astype(int)
        result_df['sentiment_negative'] = (pd.Series(sentiments) == 'negative').astype(int)
        result_df['sentiment_neutral'] = (pd.Series(sentiments) == 'neutral').astype(int)
        result_df['sentiment_label'] = sentiments

        print(f"ê°ì •ë¶„ì„ ì™„ë£Œ!")

        return result_df

# JSON ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤

def load_knusentilex_from_file(file_path):
    """
    íŒŒì¼ì—ì„œ KnuSentiLex JSON ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤

    ì´ í•¨ìˆ˜ëŠ” SentiWord_info.json íŒŒì¼ì„ ì½ì–´ì„œ ë¶„ì„ê¸°ì—ì„œ
    ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        file_path: JSON íŒŒì¼ ê²½ë¡œ

    Returns:
        JSON ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” None (ì‹¤íŒ¨ì‹œ)
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        print(f"KnuSentiLex ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(data):,}ê°œ í•­ëª©")
        return data

    except FileNotFoundError:
        print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return None
    except json.JSONDecodeError:
        print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {file_path}")
        return None
    except Exception as e:
        print(f"íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        return None

def load_knusentilex_from_text(json_text):
    """
    í…ìŠ¤íŠ¸ì—ì„œ KnuSentiLex JSON ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤

    ì´ í•¨ìˆ˜ëŠ” JSON í˜•íƒœì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ íŒŒì‹±í•˜ì—¬
    ë¶„ì„ê¸°ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        json_text: JSON í˜•íƒœì˜ í…ìŠ¤íŠ¸ ë°ì´í„°

    Returns:
        JSON ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” None (ì‹¤íŒ¨ì‹œ)
    """
    try:
        data = json.loads(json_text)
        print(f"KnuSentiLex í…ìŠ¤íŠ¸ ë°ì´í„° íŒŒì‹± ì„±ê³µ: {len(data):,}ê°œ í•­ëª©")
        return data

    except json.JSONDecodeError as e:
        print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
        return None
    except Exception as e:
        print(f"ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
        return None

# ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ ë©”ì¸ í•¨ìˆ˜ë“¤

def analyze_dataframe_with_knusentilex(df, json_data=None, text_column='full_text'):
    """
    KnuSentiLex ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°í”„ë ˆì„ì— ê°ì •ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤

    ì´ í•¨ìˆ˜ëŠ” ì‹¤ì œ KnuSentiLex ë°ì´í„°ë¥¼ í™œìš©í•œ ê³ í’ˆì§ˆ ê°ì •ë¶„ì„ì„
    í•œ ë²ˆì˜ í•¨ìˆ˜ í˜¸ì¶œë¡œ ê°„ë‹¨í•˜ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

    Args:
        df: ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„
        json_data: KnuSentiLex JSON ë°ì´í„° (ì—†ìœ¼ë©´ ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©)
        text_column: ë¶„ì„í•  í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…

    Returns:
        ê°ì •ë¶„ì„ ê²°ê³¼ê°€ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
    """
    analyzer = KnuSentiLexAnalyzer(json_data)
    return analyzer.add_sentiment_columns(df, text_column)

def get_sentiment_summary(df_with_sentiment):
    """
    ê°ì •ë¶„ì„ ê²°ê³¼ì˜ ìš”ì•½ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤

    Args:
        df_with_sentiment: ê°ì •ë¶„ì„ì´ ì™„ë£Œëœ ë°ì´í„°í”„ë ˆì„

    Returns:
        ìš”ì•½ í†µê³„ ë”•ì…”ë„ˆë¦¬
    """
    total_count = len(df_with_sentiment)
    positive_count = df_with_sentiment['sentiment_positive'].sum()
    negative_count = df_with_sentiment['sentiment_negative'].sum()
    neutral_count = df_with_sentiment['sentiment_neutral'].sum()

    return {
        'total_reviews': total_count,
        'positive_count': positive_count,
        'negative_count': negative_count,
        'neutral_count': neutral_count,
        'positive_ratio': positive_count / total_count,
        'negative_ratio': negative_count / total_count,
        'neutral_ratio': neutral_count / total_count
    }

# ì‹¤ì œ ë¦¬ë·° ë°ì´í„°ì— ì ìš©í•˜ëŠ” í•¨ìˆ˜
def apply_knusentilex_to_reviews(df, knusentilex_json_data, text_column='full_text'):
    """
    ì‹¤ì œ ë¦¬ë·° ë°ì´í„°ì— KnuSentiLex ê°ì •ë¶„ì„ì„ ì ìš©í•©ë‹ˆë‹¤

    ì´ í•¨ìˆ˜ëŠ” ì—¬ëŸ¬ë¶„ì˜ í™ˆí”Œë˜ë‹› ë¦¬ë·° ë°ì´í„°ì— ì‹¤ì œ KnuSentiLex
    ë°ì´í„°ë¥¼ ì ìš©í•˜ì—¬ ì›-í•« ì¸ì½”ë”©ëœ ê°ì •ë¶„ì„ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        df: ë¦¬ë·° ë°ì´í„°í”„ë ˆì„
        knusentilex_json_data: ì‹¤ì œ KnuSentiLex JSON ë°ì´í„°
        text_column: ë¶„ì„í•  í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…

    Returns:
        ê°ì •ë¶„ì„ ê²°ê³¼ê°€ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
    """
    print("ğŸ” ì‹¤ì œ KnuSentiLex ë°ì´í„°ë¡œ ë¦¬ë·° ê°ì •ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    # ê°ì •ë¶„ì„ ìˆ˜í–‰
    df_result = analyze_dataframe_with_knusentilex(df, knusentilex_json_data, text_column)

    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    summary = get_sentiment_summary(df_result)

    print(f"\nê°ì •ë¶„ì„ ê²°ê³¼ ìš”ì•½:")
    print(f"   ì „ì²´ ë¦¬ë·°: {summary['total_reviews']:,}ê°œ")
    print(f"   ê¸ì •: {summary['positive_count']:,}ê°œ ({summary['positive_ratio']:.1%})")
    print(f"   ë¶€ì •: {summary['negative_count']:,}ê°œ ({summary['negative_ratio']:.1%})")
    print(f"   ì¤‘ë¦½: {summary['neutral_count']:,}ê°œ ({summary['neutral_ratio']:.1%})")

    # ìƒ˜í”Œ ê²°ê³¼ ë³´ê¸°
    print(f"\nğŸ“‹ ê²°ê³¼ ìƒ˜í”Œ:")
    sample_cols = ['full_text', 'sentiment_positive', 'sentiment_negative', 'sentiment_neutral', 'sentiment_label']
    if all(col in df_result.columns for col in sample_cols):
        print(df_result[sample_cols].head())

    return df_result

json_data = load_knusentilex_from_file('data/SentiWord_info.json')
df = analyze_dataframe_with_knusentilex(df, json_data,'full_text')
#%%
df.to_csv('data/sentiment_review_data.csv', encoding='UTF-8')
#%%
# ì¬êµ¬ë§¤ ê´€ë ¨ í‚¤ì›Œë“œ ì •ì˜ (feature ì¶”ì¶œ)
repurchase_keywords = [
    'ì¬êµ¬ë§¤', 'ë‹¤ì‹œ êµ¬ë§¤', 'ë˜ êµ¬ë§¤', 'ë˜ ì‚´', 'ë‹¤ì‹œ ì‚´', 'ë‹¤ì‹œ ì‚¬',
    'ì¬ì£¼ë¬¸', 'ë‹¤ì‹œ ì£¼ë¬¸', 'ë˜ ì£¼ë¬¸', 'ì¶”ê°€ êµ¬ë§¤', 'ì¶”ê°€ë¡œ êµ¬ë§¤',
    'í•œë²ˆ ë”', 'ë˜ ì‹œì¼œ', 'ë‹¤ì‹œ ì‹œì¼œ', 'ì¬êµ¬ì…', 'ë‹¤ì‹œ êµ¬ì…',
    'ë˜ ì‚¬ê³ ', 'ë‹¤ì‹œ ì‚¬ê³ ', 'ë¦¬í”¼íŠ¸', 'ì¬ë°©ë¬¸', 'ë˜ ì´ìš©'
]

# ì¬êµ¬ë§¤ í‚¤ì›Œë“œ ì¡´ì¬ ì—¬ë¶€ë¥¼ ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ í‘œí˜„
def check_repurchase_mention(text):
    if pd.isna(text) or not isinstance(text, str):
        return 0
    text_lower = str(text).lower()
    return int(any(keyword in text_lower for keyword in repurchase_keywords))

df['repurchase_mention'] = df['full_text'].apply(check_repurchase_mention)

# ê²°ê³¼ í™•ì¸
print("ì¬êµ¬ë§¤ í‚¤ì›Œë“œ ì–¸ê¸‰ ë¶„í¬:")
print(df['repurchase_mention'].value_counts())
print(f"\nì¬êµ¬ë§¤ ì–¸ê¸‰ ë¹„ìœ¨: {df['repurchase_mention'].mean():.3f}")
#%%
# ë°°ì†¡ ê´€ë ¨ í‚¤ì›Œë“œ ì •ì˜ (feature ì¶”ì¶œ)
delivery_keywords = [
    'ë°°ì†¡', 'ë°°ë‹¬', 'íƒë°°', 'ìˆ˜ë ¹', 'ë„ì°©', 'ë°›ì•˜', 'ë°›ì•˜ì–´', 'ë°›ì•˜ìŠµë‹ˆë‹¤',
    'ë°°ì†¡ë¹„', 'ë°°ì†¡ë£Œ', 'ë¬´ë£Œë°°ì†¡', 'ë¹ ë¥¸ë°°ì†¡', 'ë‹¹ì¼ë°°ì†¡', 'ìƒˆë²½ë°°ì†¡',
    'ë°°ì†¡ì†ë„', 'ë°°ì†¡ì‹œê°„', 'ë°°ì†¡ì¼ì', 'ë°°ì†¡ì¼ì •', 'ë°°ì†¡ìƒíƒœ',
    'í¬ì¥', 'ë°•ìŠ¤', 'íŒ¨í‚¤ì§€', 'í¬ì¥ìƒíƒœ', 'í¬ì¥ì¬', 'ë½ë½ì´',
    'í•˜ë£¨ë§Œì—', 'ì´í‹€ë§Œì—', 'ë¹¨ë¦¬ ì™”', 'ë¹¨ë¦¬ ë„ì°©', 'ê¸ˆì„¸ ì™”',
    'ëŠ¦ê²Œ ì™”', 'ëŠ¦ê²Œ ë„ì°©', 'ë°°ì†¡ì§€ì—°', 'ì§€ì—°', 'ì˜¤ë˜ ê±¸ë ¸',
    'ë‹¹ì¼ ë„ì°©', 'ë‹¤ìŒë‚  ë„ì°©', 'ì£¼ë¬¸ í›„', 'ì‹œí‚¤ê³ '
]

# ë°°ì†¡ ê´€ë ¨ ì–¸ê¸‰ ì›-í•« ì¸ì½”ë”©
def check_delivery_mention(text):
    if pd.isna(text) or not isinstance(text, str):
        return 0
    text_lower = str(text).lower()
    return int(any(keyword in text_lower for keyword in delivery_keywords))

df['delivery_mention'] = df['full_text'].apply(check_delivery_mention)

# ê²°ê³¼ í™•ì¸
print("ë°°ì†¡ ê´€ë ¨ ì–¸ê¸‰ ë¶„í¬:")
print(df['delivery_mention'].value_counts())
print(f"\në°°ì†¡ ì–¸ê¸‰ ë¹„ìœ¨: {df['delivery_mention'].mean():.3f}")

# ë°°ì†¡ ì–¸ê¸‰ ìƒ˜í”Œ í™•ì¸
delivery_samples = df[df['delivery_mention'] == 1]['full_text'].head()
print(f"\në°°ì†¡ ì–¸ê¸‰ ìƒ˜í”Œ:")
for i, text in enumerate(delivery_samples, 1):
    print(f"{i}. {text[:100]}...")
#%%
# í’ˆì§ˆ ê´€ë ¨ í‚¤ì›Œë“œ ì •ì˜ (feature ì¶”ì¶œ)
quality_keywords = [
    'í’ˆì§ˆ', 'í€„ë¦¬í‹°', 'ì§ˆ', 'ë§Œë“¦ìƒˆ', 'ì œì‘', 'ë§ˆê°',
    'íŠ¼íŠ¼', 'ê²¬ê³ ', 'ë‹¨ë‹¨', 'ë¶€ì‹¤', 'í—ˆìˆ ', 'ì¡°ì¡',
    'ê³ ê¸‰', 'ì €ê¸‰', 'ì •í’ˆ', 'ì§í‰', 'ê°€ì§œ',
    'ë‚´êµ¬ì„±', 'ì˜¤ë˜', 'ê¸ˆë°© ê³ ì¥', 'ê³ ì¥', 'ë§ê°€ì ¸',
    'ì¬ì§ˆ', 'ì†Œì¬', 'í”Œë¼ìŠ¤í‹±', 'ê¸ˆì†', 'ì² ', 'ìŠ¤í…Œì¸ë¦¬ìŠ¤',
    'ë¬´ê²Œê°', 'ê°€ë²¼ì›Œ', 'ë¬´ê±°ì›Œ', 'ë¬µì§', 'íƒ„íƒ„',
    'ë§ˆê°ì²˜ë¦¬', 'ë„ìƒ‰', 'í‘œë©´', 'ê±°ì¹ ', 'ë§¤ë„ëŸ¬',
    'ì™„ì„±ë„', 'ì •ë°€ë„', 'ì •êµ', 'ì¡°ë¦½', 'ê²°í•©',
    'í”ë“¤ë ¤', 'í”ë“¤ë¦¼', 'ì•ˆì •', 'ë¶ˆì•ˆì •', 'ë‹¨ì ', 'ì¥ì '
]

# í’ˆì§ˆ ê´€ë ¨ ì–¸ê¸‰ ì›-í•« ì¸ì½”ë”©
def check_quality_mention(text):
    if pd.isna(text) or not isinstance(text, str):
        return 0
    text_lower = str(text).lower()
    return int(any(keyword in text_lower for keyword in quality_keywords))

df['quality_mention'] = df['full_text'].apply(check_quality_mention)

# ê²°ê³¼ í™•ì¸
print("í’ˆì§ˆ ê´€ë ¨ ì–¸ê¸‰ ë¶„í¬:")
print(df['quality_mention'].value_counts())
print(f"\ní’ˆì§ˆ ì–¸ê¸‰ ë¹„ìœ¨: {df['quality_mention'].mean():.3f}")

# í’ˆì§ˆ ì–¸ê¸‰ ìƒ˜í”Œ í™•ì¸
quality_samples = df[df['quality_mention'] == 1]['full_text'].head()
print(f"\ní’ˆì§ˆ ì–¸ê¸‰ ìƒ˜í”Œ:")
for i, text in enumerate(quality_samples, 1):
    print(f"{i}. {text[:100]}...")
#%%
# ì¶”ì²œ ê´€ë ¨ í‚¤ì›Œë“œ ì •ì˜ (feature ì¶”ì¶œ)
recommend_keywords = [
    'ì¶”ì²œ', 'ì¶”ì²œí•©ë‹ˆë‹¤', 'ì¶”ì²œí•´ìš”', 'ì¶”ì²œë“œë ¤', 'ê°•ì¶”', 'ê°•ë ¥ì¶”ì²œ',
    'ê¶Œí•©ë‹ˆë‹¤', 'ê¶Œí•´ìš”', 'ì‚¬ì„¸ìš”', 'ì‚¬ì‹œê¸¸', 'êµ¬ë§¤í•˜ì„¸ìš”', 'ì‚´ë§Œí•´',
    'ê´œì°®ì•„ìš”', 'ì¢‹ì•„ìš”', 'ë§Œì¡±', 'ë§Œì¡±í•´ìš”', 'ë§Œì¡±í•©ë‹ˆë‹¤',
    'ì‚¬ê¸¸ ì˜í–ˆ', 'ì˜ ìƒ€', 'ì˜ì‚°', 'ì˜ ì‚°', 'í›„íšŒ ì•ˆí•´', 'í›„íšŒì—†ì–´'
]

# ë¹„ì¶”ì²œ ê´€ë ¨ í‚¤ì›Œë“œ ì •ì˜ (feature ì¶”ì¶œ)
not_recommend_keywords = [
    'ë¹„ì¶”', 'ë¹„ì¶”ì²œ', 'ë¹„ì¶”í•©ë‹ˆë‹¤', 'ë¹„ì¶”í•´ìš”', 'ì•ˆì‚¬', 'ì‚¬ì§€ë§ˆ',
    'ì‚¬ì§€ë§ˆì„¸ìš”', 'êµ¬ë§¤í•˜ì§€ë§ˆ', 'ê¶Œí•˜ì§€ì•Šì•„', 'ê¶Œí•˜ì§€ ì•Šì•„',
    'í›„íšŒ', 'í›„íšŒí•´ìš”', 'í›„íšŒí•©ë‹ˆë‹¤', 'ì•„ê¹Œì›Œ', 'ëˆì•„ê¹Œ',
    'ì‹¤ë§', 'ì‹¤ë§í•´ìš”', 'ì‹¤ë§í–ˆì–´', 'ë³„ë¡œ', 'ë³„ë¡œì˜ˆìš”',
    'ì¶”ì²œì•ˆí•´', 'ì¶”ì²œ ì•ˆí•´', 'ê¶Œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤'
]

# ì¶”ì²œ/ë¹„ì¶”ì²œ ì›-í•« ì¸ì½”ë”©
def check_recommend_mention(text):
    if pd.isna(text) or not isinstance(text, str):
        return 0
    text_lower = str(text).lower()
    return int(any(keyword in text_lower for keyword in recommend_keywords))

def check_not_recommend_mention(text):
    if pd.isna(text) or not isinstance(text, str):
        return 0
    text_lower = str(text).lower()
    return int(any(keyword in text_lower for keyword in not_recommend_keywords))

df['recommend_mention'] = df['full_text'].apply(check_recommend_mention)
df['not_recommend_mention'] = df['full_text'].apply(check_not_recommend_mention)

# ê²°ê³¼ í™•ì¸
print("ì¶”ì²œ ì–¸ê¸‰ ë¶„í¬:")
print(df['recommend_mention'].value_counts())
print(f"ì¶”ì²œ ì–¸ê¸‰ ë¹„ìœ¨: {df['recommend_mention'].mean():.3f}")

print("\në¹„ì¶”ì²œ ì–¸ê¸‰ ë¶„í¬:")
print(df['not_recommend_mention'].value_counts())
print(f"ë¹„ì¶”ì²œ ì–¸ê¸‰ ë¹„ìœ¨: {df['not_recommend_mention'].mean():.3f}")

# ì¤‘ë³µ í™•ì¸ (ë™ì‹œì— ì¶”ì²œ/ë¹„ì¶”ì²œ ì–¸ê¸‰)
both_mention = df[(df['recommend_mention'] == 1) & (df['not_recommend_mention'] == 1)]
print(f"\nì¶”ì²œ+ë¹„ì¶”ì²œ ë™ì‹œ ì–¸ê¸‰: {len(both_mention)}ê°œ")
#%%
import re

# ê°•ì¡°í‘œí˜„ ê°œìˆ˜ ê³„ì‚° í•¨ìˆ˜ (feature ì¶”ì¶œ)
def count_emphasis_expressions(text):
    if pd.isna(text) or not isinstance(text, str):
        return 0

    count = 0

    # íŒ¨í„´ ë§¤ì¹­ (ê° íŒ¨í„´ì˜ ë°œìƒ íšŸìˆ˜)
    emphasis_patterns = [
        r'[!]{2,}',  # !!!, !!!!
        r'[?]{2,}',  # ???, ????
        r'[ã… ã…œ]{2,}',  # ã… ã… ã… , ã…œã…œã…œ
        r'[ã…]{2,}',  # ã…ã…ã…
        r'[ã…‹]{2,}',  # ã…‹ã…‹ã…‹
        r'([ê°€-í£])\1{2,}',  # ì¢‹ì•„ì•„ì•„, ìµœê³ ì˜¤ì˜¤
    ]

    for pattern in emphasis_patterns:
        matches = re.findall(pattern, text)
        count += len(matches)

    # í‚¤ì›Œë“œ ë§¤ì¹­ (ê° í‚¤ì›Œë“œì˜ ë°œìƒ íšŸìˆ˜)
    emphasis_keywords = [
        'ì •ë§', 'ì§„ì§œ', 'ì™„ì „', 'ë„ˆë¬´', 'ì—„ì²­', 'ë§¤ìš°', 'ì•„ì£¼', 'êµ‰ì¥íˆ',
        'ìµœê³ ', 'ëŒ€ë°•', 'ì©”ì–´', 'ì§±', 'í—', 'ì™€', 'ìš°ì™€', 'ì˜¤',
        'ë ˆì•Œ', 'ë¦¬ì–¼', 'ì§„ì‹¬', 'ì°', 'ì¡´', 'ê°œ', 'ì¡¸ë¼'
    ]

    text_lower = text.lower()
    for keyword in emphasis_keywords:
        count += text_lower.count(keyword)

    return count

df['emphasis_count'] = df['full_text'].apply(count_emphasis_expressions)

# ê²°ê³¼ í™•ì¸
print("ê°•ì¡°í‘œí˜„ ê°œìˆ˜ ë¶„í¬:")
print(df['emphasis_count'].value_counts().sort_index())
print(f"\ní‰ê·  ê°•ì¡°í‘œí˜„ ê°œìˆ˜: {df['emphasis_count'].mean():.2f}")
print(f"ìµœëŒ€ ê°•ì¡°í‘œí˜„ ê°œìˆ˜: {df['emphasis_count'].max()}")

# ê°•ì¡°í‘œí˜„ì´ ë§ì€ ìƒ˜í”Œ í™•ì¸
high_emphasis = df[df['emphasis_count'] >= 5].head()
print(f"\nê°•ì¡°í‘œí˜„ ë§ì€ ìƒ˜í”Œ:")
for idx, row in high_emphasis.iterrows():
    print(f"ê°œìˆ˜: {row['emphasis_count']} - {row['full_text'][:100]}...")
#%%
# ë¶„ë…¸ ê´€ë ¨ í‚¤ì›Œë“œ ì •ì˜ (feature ì¶”ì¶œ)
anger_keywords = [
    'í™”ë‚˜', 'ì§œì¦', 'ë¹¡ì³', 'ì—´ë°›', 'ì–´ì´ì—†', 'í™©ë‹¹', 'ì–´ì²˜êµ¬ë‹ˆ',
    'ë¯¸ì¹˜ê² ', 'ëŒê² ', 'ì£½ê² ', 'ë‹µë‹µ', 'ì†í„°ì ¸', 'ë¹¡ë¹¡ì´',
    'ìµœì•…', 'ìµœì €', 'ì“°ë ˆê¸°', 'ê°œíŒ', 'ì—‰ë§', 'ê°œë–¡',
    'ì–´ì´ê°€ì—†', 'ë§ì´ì•ˆë¼', 'ë§ë„ì•ˆë¼', 'í—', 'ì™€ì”¨',
    'ë¶„ë…¸', 'ë¶„í•¨', 'ì–µìš¸', 'ì–´ì´ìƒì‹¤', 'ë©˜ë¶•', 'ë¹¡ì¹¨',
    'ì—´ë»¤', 'ê°œë¹¡', 'ê°œí™”', 'ì¡´ë‚˜', 'ì§„ì§œë¡œ', 'ë ˆì•Œ'
]

def check_anger_mention(text):
    if pd.isna(text) or not isinstance(text, str):
        return 0

    text_lower = str(text).lower()

    # í‚¤ì›Œë“œ ë§¤ì¹­
    if any(keyword in text_lower for keyword in anger_keywords):
        return 1

    return 0

df['anger_mention'] = df['full_text'].apply(check_anger_mention)

# ê²°ê³¼ í™•ì¸
print("ë¶„ë…¸ í‘œí˜„ ë¶„í¬:")
print(df['anger_mention'].value_counts())
print(f"ë¶„ë…¸ í‘œí˜„ ë¹„ìœ¨: {df['anger_mention'].mean():.3f}")

# ë¶„ë…¸ í‘œí˜„ ìƒ˜í”Œ í™•ì¸
anger_samples = df[df['anger_mention'] == 1]['full_text'].head()
print(f"ë¶„ë…¸ í‘œí˜„ ìƒ˜í”Œ:")
for i, text in enumerate(anger_samples, 1):
    print(f"{i}. {text[:100]}...")

#%%
df.head(5)
#%%
# ê°€ê²© ê´€ë ¨ í‚¤ì›Œë“œ ì •ì˜ (feature ì¶”ì¶œ)
cheap_keywords = [
    'ì‹¸ë‹¤', 'ì‹¸ìš”', 'ì‹¸ì„œ', 'ì €ë ´', 'ê°€ì„±ë¹„', 'ì°©í•œê°€ê²©', 'ì €ê°€',
    'í• ì¸', 'ì„¸ì¼', 'ìŒˆ', 'ìŒ€', 'ì €ë ´í•´', 'ê°€ê²© ì¢‹', 'ê°€ê²©ì´ ì¢‹',
    'ê°€ê²© ì°©í•´', 'ê°€ê²©ì´ ì°©í•´', 'ê°€ê²©ëŒ€ë¹„', 'ê°’ì‹¸', 'ê°’ì´ ì‹¸'
]

expensive_keywords = [
    'ë¹„ì‹¸ë‹¤', 'ë¹„ì‹¸ìš”', 'ë¹„ì‹¸ì„œ', 'ë¹„ìŒˆ', 'ë¹„ì‹¼', 'ë†’ì€ ê°€ê²©',
    'ê°€ê²©ì´ ë†’', 'ê°€ê²© ë¶€ë‹´', 'ëˆì´ ë§ì´', 'ë¹„ìš©ì´ ë§ì´',
    'ê°’ë¹„ì‹¸', 'ê°’ì´ ë¹„ì‹¸', 'ê°€ê²©ëŒ€ê°€', 'ì•„ê¹Œì›Œ', 'ë¹„ì‹¸ê¸´'
]

def classify_price_mention(text):
    if pd.isna(text) or not isinstance(text, str):
        return -1  # ì–¸ê¸‰ ì—†ìŒ

    text_lower = str(text).lower()

    has_cheap = any(keyword in text_lower for keyword in cheap_keywords)
    has_expensive = any(keyword in text_lower for keyword in expensive_keywords)

    if has_cheap and has_expensive:
        return -1  # ë‘˜ ë‹¤ ì–¸ê¸‰ì‹œ ì œì™¸
    elif has_cheap:
        return 0  # ì‹¸ë‹¤
    elif has_expensive:
        return 1  # ë¹„ì‹¸ë‹¤
    else:
        return -1  # ì–¸ê¸‰ ì—†ìŒ

df['price_mention'] = df['full_text'].apply(classify_price_mention)

# ê²°ê³¼ í™•ì¸
print("ê°€ê²© ê´€ë ¨ ì–¸ê¸‰ ë¶„í¬:")
price_counts = df['price_mention'].value_counts().sort_index()
print(f"ì‹¸ë‹¤ (0): {price_counts.get(0, 0):,}ê°œ")
print(f"ë¹„ì‹¸ë‹¤ (1): {price_counts.get(1, 0):,}ê°œ")
print(f"ì–¸ê¸‰ì—†ìŒ (-1): {price_counts.get(-1, 0):,}ê°œ")

# ê°€ê²© ì–¸ê¸‰ë§Œ ìˆëŠ” ë°ì´í„° ë¹„ìœ¨
price_mentioned = df[df['price_mention'].isin([0, 1])]
if len(price_mentioned) > 0:
    cheap_ratio = (price_mentioned['price_mention'] == 0).mean()
    print(f"\nê°€ê²© ì–¸ê¸‰ ì¤‘ ì‹¸ë‹¤ ë¹„ìœ¨: {cheap_ratio:.1%}")
#%%
df.head()
#%%
df.to_csv("data/Coupang-reviews-homeplanet/review_data_complete_text_preprocessed.csv", encoding="utf-8")
#%%
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score, silhouette_samples
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì • (matplotlib)
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

# ì‚¬ìš©í•˜ì§€ ì•Šì„ í”¼ì²˜ ë¦¬ìŠ¤íŠ¸ ì •ì˜
unused_features = [
    # í…ìŠ¤íŠ¸ ë°ì´í„°
    'ìƒí’ˆëª…', 'êµ¬ë§¤ìƒí’ˆëª…', 'í—¤ë“œë¼ì¸', 'ë¦¬ë·°ë‚´ìš©', 'full_text',
    # ì‹ë³„ì ë° ë‚ ì§œ
    'êµ¬ë§¤ìëª…', 'ì‘ì„±ì¼ì',
    # ì¹´í…Œê³ ë¦¬ì»¬ ë¼ë²¨
    'sentiment_label',
    # í‰ì ì€ EDA ê²°ê³¼ ê·¸ë ‡ê²Œ í° ì˜ë¯¸ë¥¼ ê°€ì§€ì§€ ì•Šìœ¼ë¯€ë¡œ ì¼ë‹¨ ì œì™¸í•˜ê³  ì‹¤í–‰
    'í‰ì '
]

def prepare_clustering_data(df):
    """í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•œ ë°ì´í„° ì „ì²˜ë¦¬"""

    # ì‚¬ìš©í•  í”¼ì²˜ ì„ íƒ
    feature_columns = [col for col in df.columns if col not in unused_features]

    print(f"ì „ì²´ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
    print(f"ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì»¬ëŸ¼ ìˆ˜: {len(unused_features)}")
    print(f"í´ëŸ¬ìŠ¤í„°ë§ì— ì‚¬ìš©í•  ì»¬ëŸ¼ ìˆ˜: {len(feature_columns)}")
    print(f"\nì‚¬ìš©í•  í”¼ì²˜ë“¤:")
    for i, col in enumerate(feature_columns, 1):
        print(f"{i:2d}. {col}")

    # í”¼ì²˜ ë°ì´í„° ì¶”ì¶œ
    X = df[feature_columns].copy()

    # price_mentionì˜ ê²½ìš° -1, 0, 1 ê°’ì„ ê°€ì§€ë¯€ë¡œ ì›í•« ì¸ì½”ë”©
    if 'price_mention' in X.columns:
        # price_mentionì„ ì›í•« ì¸ì½”ë”©
        price_dummies = pd.get_dummies(X['price_mention'], prefix='price')
        X = pd.concat([X.drop('price_mention', axis=1), price_dummies], axis=1)

    # ê²°ì¸¡ê°’ í™•ì¸ ë° ì²˜ë¦¬
    missing_values = X.isnull().sum()
    if missing_values.sum() > 0:
        print(f"\nê²°ì¸¡ê°’ ë°œê²¬:")
        print(missing_values[missing_values > 0])
        X = X.fillna(0)  # ê²°ì¸¡ê°’ì„ 0ìœ¼ë¡œ ì±„ì›€

    print(f"\nìµœì¢… í”¼ì²˜ í˜•íƒœ: {X.shape}")
    print(f"ìµœì¢… í”¼ì²˜ ëª©ë¡: {list(X.columns)}")

    return X

X = prepare_clustering_data(df)
#%%
def apply_pca_reduction(X, n_components=3):
    """PCAë¥¼ ì‚¬ìš©í•œ ì°¨ì› ì¶•ì†Œ"""

    # ë°ì´í„° í‘œì¤€í™”
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # PCA ì ìš©
    pca = PCA(n_components=n_components)
    X_pca = pca.fit_transform(X_scaled)

    # ì„¤ëª…ëœ ë¶„ì‚°ë¹„ ì¶œë ¥
    explained_variance_ratio = pca.explained_variance_ratio_
    cumulative_variance_ratio = np.cumsum(explained_variance_ratio)

    print(f"PCA ê²°ê³¼:")
    for i in range(n_components):
        print(f"PC{i+1}: {explained_variance_ratio[i]:.3f} ({explained_variance_ratio[i]*100:.1f}%)")
    print(f"ëˆ„ì  ì„¤ëª… ë¶„ì‚°ë¹„: {cumulative_variance_ratio[-1]:.3f} ({cumulative_variance_ratio[-1]*100:.1f}%)")

    # ì£¼ì„±ë¶„ë³„ ê¸°ì—¬ë„ê°€ ë†’ì€ ì›ë³¸ í”¼ì²˜ í™•ì¸
    print(f"\nê° ì£¼ì„±ë¶„ë³„ ì£¼ìš” í”¼ì²˜ (ìƒìœ„ 5ê°œ):")
    feature_names = X.columns if hasattr(X, 'columns') else [f'feature_{i}' for i in range(X.shape[1])]

    for i in range(n_components):
        pc_loadings = pca.components_[i]
        # ì ˆëŒ“ê°’ì´ í° ìˆœì„œë¡œ ì •ë ¬
        top_features_idx = np.argsort(np.abs(pc_loadings))[::-1][:5]
        print(f"\nPC{i+1} ì£¼ìš” í”¼ì²˜:")
        for j, idx in enumerate(top_features_idx):
            print(f"  {j+1}. {feature_names[idx]}: {pc_loadings[idx]:.3f}")

    return X_pca, scaler, pca

def visualize_pca_variance(pca, max_components=10):
    """PCA ë¶„ì‚° ì„¤ëª…ë¹„ ì‹œê°í™”"""

    n_components = min(len(pca.explained_variance_ratio_), max_components)

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

    # ê°œë³„ ë¶„ì‚° ì„¤ëª…ë¹„
    ax1.bar(range(1, n_components+1), pca.explained_variance_ratio_[:n_components])
    ax1.set_xlabel('Principal Component')
    ax1.set_ylabel('Explained Variance Ratio')
    ax1.set_title('Individual Explained Variance')
    ax1.set_xticks(range(1, n_components+1))

    # ëˆ„ì  ë¶„ì‚° ì„¤ëª…ë¹„
    cumsum_var = np.cumsum(pca.explained_variance_ratio_[:n_components])
    ax2.plot(range(1, n_components+1), cumsum_var, 'bo-')
    ax2.set_xlabel('Number of Components')
    ax2.set_ylabel('Cumulative Explained Variance')
    ax2.set_title('Cumulative Explained Variance')
    ax2.set_xticks(range(1, n_components+1))
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

X_pca, scaler, pca = apply_pca_reduction(X, n_components=3)
visualize_pca_variance(pca)
#%%
def optimize_kmeans(X_pca, k_range=range(3, 11)):
    """K-means í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"""

    print(f"K-means í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (k={k_range.start}~{k_range.stop-1})")
    print("-" * 50)

    best_score = -1
    best_k = 2
    scores = []
    inertias = []

    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(X_pca)

        if len(np.unique(cluster_labels)) > 1:  # í´ëŸ¬ìŠ¤í„°ê°€ 1ê°œê°€ ì•„ë‹Œ ê²½ìš°ë§Œ
            score = silhouette_score(X_pca, cluster_labels)
            scores.append(score)
            inertias.append(kmeans.inertia_)
            print(f"k={k}: ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ = {score:.4f}, Inertia = {kmeans.inertia_:.2f}")

            if score > best_score:
                best_score = score
                best_k = k
        else:
            scores.append(-1)
            inertias.append(0)
            print(f"k={k}: í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨")

    print(f"\nìµœì  K-means íŒŒë¼ë¯¸í„°: k={best_k}, ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´={best_score:.4f}")

    return best_k, best_score, scores, inertias

def plot_kmeans_scores(k_range, scores, inertias):
    """K-means ì„±ëŠ¥ ì§€í‘œ ì‹œê°í™”"""

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

    # ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´
    valid_scores = [s for s in scores if s != -1]
    valid_k = [k for k, s in zip(k_range, scores) if s != -1]

    ax1.plot(valid_k, valid_scores, 'bo-')
    ax1.set_xlabel('Number of Clusters (k)')
    ax1.set_ylabel('Silhouette Score')
    ax1.set_title('Silhouette Score vs Number of Clusters')
    ax1.grid(True, alpha=0.3)
    ax1.set_xticks(valid_k)

    # Elbow Method (Inertia)
    valid_inertias = [i for i in inertias if i != 0]
    ax2.plot(valid_k, valid_inertias, 'ro-')
    ax2.set_xlabel('Number of Clusters (k)')
    ax2.set_ylabel('Inertia')
    ax2.set_title('Elbow Method (Inertia vs Number of Clusters)')
    ax2.grid(True, alpha=0.3)
    ax2.set_xticks(valid_k)

    plt.tight_layout()
    plt.show()

def perform_final_kmeans(X_pca, best_k):
    """ìµœì  kë¡œ ìµœì¢… K-means ìˆ˜í–‰"""

    print(f"ìµœì  k={best_k}ë¡œ ìµœì¢… K-means í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰")

    kmeans_final = KMeans(n_clusters=best_k, random_state=42, n_init=10)
    kmeans_labels = kmeans_final.fit_predict(X_pca)

    # í´ëŸ¬ìŠ¤í„°ë³„ í¬ê¸° í™•ì¸
    unique_labels, counts = np.unique(kmeans_labels, return_counts=True)
    print(f"\ní´ëŸ¬ìŠ¤í„°ë³„ í¬ê¸°:")
    for label, count in zip(unique_labels, counts):
        print(f"  í´ëŸ¬ìŠ¤í„° {label}: {count:,}ê°œ ({count/len(X_pca)*100:.1f}%)")

    # ìµœì¢… ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´
    final_score = silhouette_score(X_pca, kmeans_labels)
    print(f"\nìµœì¢… ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´: {final_score:.4f}")

    return kmeans_final, kmeans_labels, final_score

best_k, best_score, scores, inertias = optimize_kmeans(X_pca)
plot_kmeans_scores(range(3, 11), scores, inertias)
kmeans_model, kmeans_labels, final_score = perform_final_kmeans(X_pca, best_k)
#%%
def optimize_dbscan(X_pca, eps_range=np.arange(0.1, 1.6, 0.2), min_samples_range=range(2, 30, 1)):
    """DBSCAN í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"""

    print(f"DBSCAN í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
    print(f"eps ë²”ìœ„: {eps_range[0]:.1f}~{eps_range[-1]:.1f}")
    print(f"min_samples ë²”ìœ„: {min_samples_range.start}~{min_samples_range.stop-1}")
    print("-" * 50)

    best_score = -1
    best_params = {'eps': 0.5, 'min_samples': 5}
    results = []

    # ìƒ˜í”Œë§ìœ¼ë¡œ íŠœë‹ ì†ë„ í–¥ìƒ (ë°ì´í„°ê°€ í° ê²½ìš°)
    if len(X_pca) > 10000:
        sample_idx = np.random.choice(len(X_pca), 10000, replace=False)
        X_sample = X_pca[sample_idx]
        print(f"í° ë°ì´í„°ì…‹ìœ¼ë¡œ ì¸í•´ {len(X_sample)}ê°œ ìƒ˜í”Œë¡œ íŠœë‹ ìˆ˜í–‰")
    else:
        X_sample = X_pca

    for eps in eps_range:
        for min_samples in min_samples_range:
            dbscan = DBSCAN(eps=eps, min_samples=min_samples)
            cluster_labels = dbscan.fit_predict(X_sample)

            # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸(-1) ì œì™¸í•˜ê³  ì‹¤ì œ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ í™•ì¸
            unique_labels = np.unique(cluster_labels)
            n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
            n_noise = np.sum(cluster_labels == -1)

            if n_clusters > 1 and n_noise < len(X_sample) * 0.9:  # í´ëŸ¬ìŠ¤í„°ê°€ ì¡´ì¬í•˜ê³  ë…¸ì´ì¦ˆê°€ 90% ë¯¸ë§Œ
                score = silhouette_score(X_sample, cluster_labels)
                results.append({
                    'eps': eps,
                    'min_samples': min_samples,
                    'n_clusters': n_clusters,
                    'n_noise': n_noise,
                    'noise_ratio': n_noise / len(X_sample),
                    'silhouette_score': score
                })

                if score > best_score:
                    best_score = score
                    best_params = {'eps': eps, 'min_samples': min_samples}

    # ìƒìœ„ 5ê°œ ê²°ê³¼ ì¶œë ¥
    if results:
        results_df = pd.DataFrame(results)
        results_df = results_df.sort_values('silhouette_score', ascending=False)

        print("ìƒìœ„ 5ê°œ DBSCAN ê²°ê³¼:")
        print(results_df.head().to_string(index=False, float_format='%.4f'))
        print(f"\nìµœì  DBSCAN íŒŒë¼ë¯¸í„°: eps={best_params['eps']:.1f}, min_samples={best_params['min_samples']}")
        print(f"ìµœê³  ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´: {best_score:.4f}")
    else:
        print("ìœ íš¨í•œ DBSCAN ê²°ê³¼ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        best_params = {'eps': 0.5, 'min_samples': 5}
        best_score = -1

    return best_params, best_score, results

def perform_final_dbscan(X_pca, best_params):
    """ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… DBSCAN ìˆ˜í–‰"""

    print(f"ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… DBSCAN í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰")
    print(f"eps={best_params['eps']:.1f}, min_samples={best_params['min_samples']}")

    dbscan_final = DBSCAN(eps=best_params['eps'], min_samples=best_params['min_samples'])
    dbscan_labels = dbscan_final.fit_predict(X_pca)

    # í´ëŸ¬ìŠ¤í„°ë³„ í¬ê¸° í™•ì¸
    unique_labels, counts = np.unique(dbscan_labels, return_counts=True)
    print(f"\ní´ëŸ¬ìŠ¤í„°ë³„ í¬ê¸°:")
    for label, count in zip(unique_labels, counts):
        if label == -1:
            print(f"  ë…¸ì´ì¦ˆ: {count:,}ê°œ ({count/len(X_pca)*100:.1f}%)")
        else:
            print(f"  í´ëŸ¬ìŠ¤í„° {label}: {count:,}ê°œ ({count/len(X_pca)*100:.1f}%)")

    # ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ (ë…¸ì´ì¦ˆ ì œì™¸)
    if len(np.unique(dbscan_labels)) > 1:
        final_score = silhouette_score(X_pca, dbscan_labels)
        print(f"\nìµœì¢… ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´: {final_score:.4f}")
    else:
        final_score = -1
        print(f"\nìœ íš¨í•œ í´ëŸ¬ìŠ¤í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    return dbscan_final, dbscan_labels, final_score

def plot_dbscan_results(results_df):
    """DBSCAN íŠœë‹ ê²°ê³¼ ì‹œê°í™”"""

    if len(results_df) == 0:
        print("ì‹œê°í™”í•  DBSCAN ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))

    # eps vs ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´
    ax1.scatter(results_df['eps'], results_df['silhouette_score'], alpha=0.6)
    ax1.set_xlabel('eps')
    ax1.set_ylabel('Silhouette Score')
    ax1.set_title('eps vs Silhouette Score')
    ax1.grid(True, alpha=0.3)

    # min_samples vs ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´
    ax2.scatter(results_df['min_samples'], results_df['silhouette_score'], alpha=0.6)
    ax2.set_xlabel('min_samples')
    ax2.set_ylabel('Silhouette Score')
    ax2.set_title('min_samples vs Silhouette Score')
    ax2.grid(True, alpha=0.3)

    # í´ëŸ¬ìŠ¤í„° ìˆ˜ vs ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´
    ax3.scatter(results_df['n_clusters'], results_df['silhouette_score'], alpha=0.6)
    ax3.set_xlabel('Number of Clusters')
    ax3.set_ylabel('Silhouette Score')
    ax3.set_title('Clusters vs Silhouette Score')
    ax3.grid(True, alpha=0.3)

    # ë…¸ì´ì¦ˆ ë¹„ìœ¨ vs ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´
    ax4.scatter(results_df['noise_ratio'], results_df['silhouette_score'], alpha=0.6)
    ax4.set_xlabel('Noise Ratio')
    ax4.set_ylabel('Silhouette Score')
    ax4.set_title('Noise Ratio vs Silhouette Score')
    ax4.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

best_params, best_score, results = optimize_dbscan(X_pca)
if results:
    results_df = pd.DataFrame(results)
    plot_dbscan_results(results_df)
dbscan_model, dbscan_labels, final_score = perform_final_dbscan(X_pca, best_params)
#%%
def visualize_clusters_3d(X_pca, kmeans_labels, dbscan_labels, best_k, best_dbscan_params):
    """3D í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™”"""

    fig = plt.figure(figsize=(15, 6))

    # K-means ê²°ê³¼
    ax1 = fig.add_subplot(121, projection='3d')
    scatter1 = ax1.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2],
                          c=kmeans_labels, cmap='tab10', alpha=0.6, s=20)
    ax1.set_title(f'K-means Clustering (k={best_k})', fontsize=12)
    ax1.set_xlabel('PC1')
    ax1.set_ylabel('PC2')
    ax1.set_zlabel('PC3')

    # DBSCAN ê²°ê³¼
    ax2 = fig.add_subplot(122, projection='3d')
    scatter2 = ax2.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2],
                          c=dbscan_labels, cmap='tab10', alpha=0.6, s=20)
    ax2.set_title(f'DBSCAN Clustering (eps={best_dbscan_params["eps"]:.1f}, min_samples={best_dbscan_params["min_samples"]})',
                  fontsize=12)
    ax2.set_xlabel('PC1')
    ax2.set_ylabel('PC2')
    ax2.set_zlabel('PC3')

    plt.tight_layout()
    plt.show()

def visualize_clusters_2d(X_pca, kmeans_labels, dbscan_labels, best_k, best_dbscan_params):
    """2D í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™” (PC1-PC2, PC1-PC3, PC2-PC3)"""

    fig, axes = plt.subplots(2, 3, figsize=(18, 10))

    # K-means ê²°ê³¼
    # PC1 vs PC2
    axes[0,0].scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels, cmap='tab10', alpha=0.6, s=20)
    axes[0,0].set_xlabel('PC1')
    axes[0,0].set_ylabel('PC2')
    axes[0,0].set_title(f'K-means: PC1 vs PC2 (k={best_k})')

    # PC1 vs PC3
    axes[0,1].scatter(X_pca[:, 0], X_pca[:, 2], c=kmeans_labels, cmap='tab10', alpha=0.6, s=20)
    axes[0,1].set_xlabel('PC1')
    axes[0,1].set_ylabel('PC3')
    axes[0,1].set_title(f'K-means: PC1 vs PC3 (k={best_k})')

    # PC2 vs PC3
    axes[0,2].scatter(X_pca[:, 1], X_pca[:, 2], c=kmeans_labels, cmap='tab10', alpha=0.6, s=20)
    axes[0,2].set_xlabel('PC2')
    axes[0,2].set_ylabel('PC3')
    axes[0,2].set_title(f'K-means: PC2 vs PC3 (k={best_k})')

    # DBSCAN ê²°ê³¼
    # PC1 vs PC2
    axes[1,0].scatter(X_pca[:, 0], X_pca[:, 1], c=dbscan_labels, cmap='tab10', alpha=0.6, s=20)
    axes[1,0].set_xlabel('PC1')
    axes[1,0].set_ylabel('PC2')
    axes[1,0].set_title(f'DBSCAN: PC1 vs PC2 (eps={best_dbscan_params["eps"]:.1f})')

    # PC1 vs PC3
    axes[1,1].scatter(X_pca[:, 0], X_pca[:, 2], c=dbscan_labels, cmap='tab10', alpha=0.6, s=20)
    axes[1,1].set_xlabel('PC1')
    axes[1,1].set_ylabel('PC3')
    axes[1,1].set_title(f'DBSCAN: PC1 vs PC3 (eps={best_dbscan_params["eps"]:.1f})')

    # PC2 vs PC3
    axes[1,2].scatter(X_pca[:, 1], X_pca[:, 2], c=dbscan_labels, cmap='tab10', alpha=0.6, s=20)
    axes[1,2].set_xlabel('PC2')
    axes[1,2].set_ylabel('PC3')
    axes[1,2].set_title(f'DBSCAN: PC2 vs PC3 (eps={best_dbscan_params["eps"]:.1f})')

    plt.tight_layout()
    plt.show()

def plot_silhouette_analysis(X_pca, cluster_labels, method_name):
    """ì‹¤ë£¨ì—£ ë¶„ì„ í”Œë¡¯"""

    if len(np.unique(cluster_labels)) < 2:
        print(f"{method_name}: í´ëŸ¬ìŠ¤í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    silhouette_avg = silhouette_score(X_pca, cluster_labels)
    sample_silhouette_values = silhouette_samples(X_pca, cluster_labels)

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

    # ì‹¤ë£¨ì—£ í”Œë¡¯
    y_lower = 10
    unique_labels = np.unique(cluster_labels)

    for i, label in enumerate(unique_labels):
        if label == -1:  # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ëŠ” ì œì™¸
            continue

        cluster_silhouette_values = sample_silhouette_values[cluster_labels == label]
        cluster_silhouette_values.sort()

        size_cluster_i = cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = plt.cm.nipy_spectral(float(i) / len(unique_labels))
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                         0, cluster_silhouette_values,
                         facecolor=color, edgecolor=color, alpha=0.7)

        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(label))
        y_lower = y_upper + 10

    ax1.set_xlabel('Silhouette coefficient values')
    ax1.set_ylabel('Cluster label')
    ax1.set_title(f'{method_name} Silhouette Analysis\n(Average Score: {silhouette_avg:.3f})')

    # í‰ê·  ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ ì„ 
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    # í´ëŸ¬ìŠ¤í„°ë³„ ë¶„í¬
    if -1 in cluster_labels:  # DBSCANì˜ ê²½ìš° ë…¸ì´ì¦ˆ ì œì™¸
        mask = cluster_labels != -1
        scatter = ax2.scatter(X_pca[mask, 0], X_pca[mask, 1],
                            c=cluster_labels[mask], cmap='tab10', alpha=0.6, s=20)
    else:
        scatter = ax2.scatter(X_pca[:, 0], X_pca[:, 1],
                            c=cluster_labels, cmap='tab10', alpha=0.6, s=20)

    ax2.set_xlabel('PC1')
    ax2.set_ylabel('PC2')
    ax2.set_title(f'{method_name} Clusters')

    plt.tight_layout()
    plt.show()

visualize_clusters_3d(X_pca, kmeans_labels, dbscan_labels, best_k, best_params)
visualize_clusters_2d(X_pca, kmeans_labels, dbscan_labels, best_k, best_params)
plot_silhouette_analysis(X_pca, kmeans_labels, "K-means")
plot_silhouette_analysis(X_pca, dbscan_labels, "DBSCAN")
#%%
def analyze_cluster_characteristics(df, kmeans_labels, dbscan_labels):
    """í´ëŸ¬ìŠ¤í„°ë³„ íŠ¹ì„± ë¶„ì„"""

    print("="*60)
    print("í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ë¶„ì„")
    print("="*60)

    # K-means í´ëŸ¬ìŠ¤í„° ë¶„ì„
    df_kmeans = df.copy()
    df_kmeans['cluster'] = kmeans_labels

    print(f"\n1. K-means í´ëŸ¬ìŠ¤í„°ë³„ íŠ¹ì„±:")
    print("-" * 40)

    # í´ëŸ¬ìŠ¤í„°ë³„ í¬ê¸°
    cluster_sizes = df_kmeans['cluster'].value_counts().sort_index()
    print("í´ëŸ¬ìŠ¤í„°ë³„ í¬ê¸°:")
    for cluster, size in cluster_sizes.items():
        print(f"  í´ëŸ¬ìŠ¤í„° {cluster}: {size:,}ê°œ ({size/len(df)*100:.1f}%)")

    # ì£¼ìš” í”¼ì²˜ë³„ í‰ê· ê°’ ë¹„êµ
    key_features = ['í‰ì ', 'full_text_length', 'emoticon_count', 'emphasis_count',
                   'sentiment_positive', 'sentiment_negative', 'sentiment_neutral',
                   'recommend_mention', 'not_recommend_mention', 'repurchase_mention',
                   'delivery_mention', 'quality_mention', 'anger_mention']

    available_features = [f for f in key_features if f in df.columns]

    if available_features:
        print(f"\nì£¼ìš” í”¼ì²˜ë³„ í´ëŸ¬ìŠ¤í„° í‰ê· :")
        cluster_stats = df_kmeans.groupby('cluster')[available_features].mean()
        print(cluster_stats.round(3))

    # DBSCAN í´ëŸ¬ìŠ¤í„° ë¶„ì„
    df_dbscan = df.copy()
    df_dbscan['cluster'] = dbscan_labels

    print(f"\n2. DBSCAN í´ëŸ¬ìŠ¤í„°ë³„ íŠ¹ì„±:")
    print("-" * 40)

    # í´ëŸ¬ìŠ¤í„°ë³„ í¬ê¸° (ë…¸ì´ì¦ˆ í¬í•¨)
    cluster_sizes_dbscan = df_dbscan['cluster'].value_counts().sort_index()
    print("í´ëŸ¬ìŠ¤í„°ë³„ í¬ê¸°:")
    for cluster, size in cluster_sizes_dbscan.items():
        if cluster == -1:
            print(f"  ë…¸ì´ì¦ˆ: {size:,}ê°œ ({size/len(df)*100:.1f}%)")
        else:
            print(f"  í´ëŸ¬ìŠ¤í„° {cluster}: {size:,}ê°œ ({size/len(df)*100:.1f}%)")

    if available_features:
        print(f"\nì£¼ìš” í”¼ì²˜ë³„ í´ëŸ¬ìŠ¤í„° í‰ê·  (ë…¸ì´ì¦ˆ ì œì™¸):")
        cluster_stats_dbscan = df_dbscan[df_dbscan['cluster'] != -1].groupby('cluster')[available_features].mean()
        if len(cluster_stats_dbscan) > 0:
            print(cluster_stats_dbscan.round(3))

    return df_kmeans, df_dbscan

def detailed_cluster_profiling(df_with_clusters, cluster_column='cluster', method_name=''):
    """ìƒì„¸í•œ í´ëŸ¬ìŠ¤í„° í”„ë¡œíŒŒì¼ë§"""

    print(f"\n{method_name} ìƒì„¸ í´ëŸ¬ìŠ¤í„° í”„ë¡œíŒŒì¼ë§:")
    print("="*50)

    # í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ë°˜ë³µ
    unique_clusters = sorted(df_with_clusters[cluster_column].unique())

    for cluster in unique_clusters:
        if cluster == -1:
            print(f"\nğŸ“ ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ ë¶„ì„:")
        else:
            print(f"\nğŸ“ í´ëŸ¬ìŠ¤í„° {cluster} ë¶„ì„:")
        print("-" * 30)

        cluster_data = df_with_clusters[df_with_clusters[cluster_column] == cluster]

        # ê¸°ë³¸ í†µê³„
        print(f"í¬ê¸°: {len(cluster_data):,}ê°œ ({len(cluster_data)/len(df_with_clusters)*100:.1f}%)")

        # í‰ì  ë¶„í¬
        if 'í‰ì ' in cluster_data.columns:
            rating_dist = cluster_data['í‰ì '].value_counts().sort_index()
            print(f"í‰ì  ë¶„í¬: {dict(rating_dist)}")
            print(f"í‰ê·  í‰ì : {cluster_data['í‰ì '].mean():.2f}")

        # ê°ì • ë¶„í¬
        sentiment_cols = ['sentiment_positive', 'sentiment_negative', 'sentiment_neutral']
        available_sentiment = [col for col in sentiment_cols if col in cluster_data.columns]
        if available_sentiment:
            sentiment_counts = cluster_data[available_sentiment].sum()
            print(f"ê°ì • ë¶„í¬: ê¸ì • {sentiment_counts.get('sentiment_positive', 0)}, "
                  f"ë¶€ì • {sentiment_counts.get('sentiment_negative', 0)}, "
                  f"ì¤‘ë¦½ {sentiment_counts.get('sentiment_neutral', 0)}")

        # íŠ¹ì§•ì ì¸ í‚¤ì›Œë“œ ì–¸ê¸‰ë¥ 
        keyword_features = ['recommend_mention', 'repurchase_mention', 'delivery_mention',
                          'quality_mention', 'anger_mention']
        available_keywords = [col for col in keyword_features if col in cluster_data.columns]
        if available_keywords:
            keyword_rates = cluster_data[available_keywords].mean()
            print("í‚¤ì›Œë“œ ì–¸ê¸‰ë¥ :")
            for col in available_keywords:
                feature_name = col.replace('_mention', '')
                print(f"  {feature_name}: {keyword_rates[col]:.1%}")

        # í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ê°•ì¡° í‘œí˜„
        if 'full_text_length' in cluster_data.columns:
            print(f"í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´: {cluster_data['full_text_length'].mean():.0f}ì")
        if 'emphasis_count' in cluster_data.columns:
            print(f"í‰ê·  ê°•ì¡° í‘œí˜„: {cluster_data['emphasis_count'].mean():.1f}ê°œ")

def compare_clusters_statistically(df_kmeans, df_dbscan):
    """í´ëŸ¬ìŠ¤í„° ê°„ í†µê³„ì  ë¹„êµ"""

    print("\ní†µê³„ì  í´ëŸ¬ìŠ¤í„° ë¹„êµ:")
    print("="*40)

    # ìˆ˜ì¹˜í˜• í”¼ì²˜ë“¤ ì„ íƒ
    numeric_features = df_kmeans.select_dtypes(include=[np.number]).columns
    numeric_features = [col for col in numeric_features if col != 'cluster']

    # K-means í´ëŸ¬ìŠ¤í„° ê°„ ë¶„ì‚° ë¶„ì„
    print("\nK-means í´ëŸ¬ìŠ¤í„° ê°„ íŠ¹ì„± ì°¨ì´ (í‘œì¤€í¸ì°¨):")
    kmeans_cluster_std = df_kmeans.groupby('cluster')[numeric_features].std().mean(axis=1)
    print(kmeans_cluster_std.round(3))

    # DBSCAN í´ëŸ¬ìŠ¤í„° ê°„ ë¶„ì‚° ë¶„ì„ (ë…¸ì´ì¦ˆ ì œì™¸)
    df_dbscan_no_noise = df_dbscan[df_dbscan['cluster'] != -1]
    if len(df_dbscan_no_noise) > 0:
        print("\nDBSCAN í´ëŸ¬ìŠ¤í„° ê°„ íŠ¹ì„± ì°¨ì´ (í‘œì¤€í¸ì°¨):")
        dbscan_cluster_std = df_dbscan_no_noise.groupby('cluster')[numeric_features].std().mean(axis=1)
        print(dbscan_cluster_std.round(3))

def save_cluster_results(df_kmeans, df_dbscan, filename_prefix='cluster_results'):
    """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì €ì¥"""

    # K-means ê²°ê³¼ ì €ì¥
    kmeans_file = f"{filename_prefix}_kmeans.csv"
    df_kmeans.to_csv(kmeans_file, index=False, encoding='utf-8')
    print(f"K-means ê²°ê³¼ ì €ì¥: {kmeans_file}")

    # DBSCAN ê²°ê³¼ ì €ì¥
    dbscan_file = f"{filename_prefix}_dbscan.csv"
    df_dbscan.to_csv(dbscan_file, index=False, encoding='utf-8')
    print(f"DBSCAN ê²°ê³¼ ì €ì¥: {dbscan_file}")

    # í´ëŸ¬ìŠ¤í„°ë³„ ìš”ì•½ ì €ì¥
    summary_data = []

    # K-means ìš”ì•½
    for cluster in sorted(df_kmeans['cluster'].unique()):
        cluster_data = df_kmeans[df_kmeans['cluster'] == cluster]
        summary_data.append({
            'method': 'K-means',
            'cluster': cluster,
            'size': len(cluster_data),
            'percentage': len(cluster_data) / len(df_kmeans) * 100,
            'avg_rating': cluster_data['í‰ì '].mean() if 'í‰ì ' in cluster_data.columns else None,
            'avg_text_length': cluster_data['full_text_length'].mean() if 'full_text_length' in cluster_data.columns else None
        })

    # DBSCAN ìš”ì•½
    for cluster in sorted(df_dbscan['cluster'].unique()):
        cluster_data = df_dbscan[df_dbscan['cluster'] == cluster]
        summary_data.append({
            'method': 'DBSCAN',
            'cluster': cluster,
            'size': len(cluster_data),
            'percentage': len(cluster_data) / len(df_dbscan) * 100,
            'avg_rating': cluster_data['í‰ì '].mean() if 'í‰ì ' in cluster_data.columns else None,
            'avg_text_length': cluster_data['full_text_length'].mean() if 'full_text_length' in cluster_data.columns else None
        })

    summary_df = pd.DataFrame(summary_data)
    summary_file = f"{filename_prefix}_summary.csv"
    summary_df.to_csv(summary_file, index=False, encoding='utf-8')
    print(f"í´ëŸ¬ìŠ¤í„° ìš”ì•½ ì €ì¥: {summary_file}")

# ì‹¤í–‰ ì˜ˆì‹œ
df_kmeans, df_dbscan = analyze_cluster_characteristics(df, kmeans_labels, dbscan_labels)
detailed_cluster_profiling(df_kmeans, 'cluster', 'K-means')
detailed_cluster_profiling(df_dbscan, 'cluster', 'DBSCAN')
compare_clusters_statistically(df_kmeans, df_dbscan)
save_cluster_results(df_kmeans, df_dbscan)
#%%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from scipy.spatial.distance import pdist
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
import matplotlib.animation as animation
from PIL import Image
import io
import warnings
warnings.filterwarnings('ignore')

def load_and_prepare_data(file_path='cluster_results_kmeans.csv'):
    """í´ëŸ¬ìŠ¤í„° ê²°ê³¼ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""

    df = pd.read_csv(file_path)
    print(f"ë°ì´í„° í¬ê¸°: {df.shape}")

    # í´ëŸ¬ìŠ¤í„°ë§ì— ì‚¬ìš©í•  í”¼ì²˜ ì„ íƒ (í…ìŠ¤íŠ¸ ë° ì‹ë³„ì ì œì™¸)
    exclude_cols = ['ìƒí’ˆëª…', 'êµ¬ë§¤ìƒí’ˆëª…', 'í—¤ë“œë¼ì¸', 'ë¦¬ë·°ë‚´ìš©', 'full_text',
                   'êµ¬ë§¤ìëª…', 'ì‘ì„±ì¼ì', 'sentiment_label', 'cluster']

    feature_cols = [col for col in df.columns if col not in exclude_cols]
    X = df[feature_cols].copy()

    # price_mention ì›í•« ì¸ì½”ë”© (ìˆë‹¤ë©´)
    if 'price_mention' in X.columns:
        price_dummies = pd.get_dummies(X['price_mention'], prefix='price')
        X = pd.concat([X.drop('price_mention', axis=1), price_dummies], axis=1)

    # ê²°ì¸¡ê°’ ì²˜ë¦¬
    X = X.fillna(0)

    # í´ëŸ¬ìŠ¤í„° ë¼ë²¨
    cluster_labels = df['cluster'].values

    print(f"í”¼ì²˜ ìˆ˜: {X.shape[1]}")
    print(f"í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(np.unique(cluster_labels))}")

    return X, cluster_labels, df

def apply_tsne_3d(X, cluster_labels, perplexity=30, n_iter=1000, random_state=42):
    """t-SNE 3D ì°¨ì› ì¶•ì†Œ"""

    print(f"t-SNE 3D ë³€í™˜ ì‹œì‘...")
    print(f"íŒŒë¼ë¯¸í„°: perplexity={perplexity}, n_iter={n_iter}")

    # ë°ì´í„° í‘œì¤€í™”
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # í° ë°ì´í„°ì…‹ì¸ ê²½ìš° ìƒ˜í”Œë§
    if len(X_scaled) > 10000:
        sample_idx = np.random.choice(len(X_scaled), 10000, replace=False)
        X_sample = X_scaled[sample_idx]
        labels_sample = cluster_labels[sample_idx]
        print(f"ìƒ˜í”Œë§: {len(X_sample)}ê°œ ì‚¬ìš©")
    else:
        X_sample = X_scaled
        labels_sample = cluster_labels

    # t-SNE ì ìš©
    tsne = TSNE(n_components=3, perplexity=perplexity, n_iter=n_iter,
                random_state=random_state, learning_rate='auto', init='random')
    X_tsne = tsne.fit_transform(X_sample)

    print(f"t-SNE ì™„ë£Œ! KL divergence: {tsne.kl_divergence_:.4f}")

    return X_tsne, labels_sample, scaler, tsne

def plot_static_3d(X_tsne, labels, title="t-SNE 3D Clustering"):
    """ì •ì  3D í”Œë¡¯"""

    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')

    # í´ëŸ¬ìŠ¤í„°ë³„ ìƒ‰ìƒ
    unique_labels = np.unique(labels)
    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))

    for i, label in enumerate(unique_labels):
        mask = labels == label
        ax.scatter(X_tsne[mask, 0], X_tsne[mask, 1], X_tsne[mask, 2],
                  c=[colors[i]], label=f'Cluster {label}', alpha=0.7, s=20)

    ax.set_xlabel('t-SNE 1')
    ax.set_ylabel('t-SNE 2')
    ax.set_zlabel('t-SNE 3')
    ax.set_title(title)
    ax.legend()

    plt.tight_layout()
    plt.show()

def create_rotating_gif(X_tsne, labels, filename='tsne_3d_rotating.gif',
                       duration=100, elevation=20):
    """íšŒì „í•˜ëŠ” 3D í”Œë¡¯ GIF ìƒì„±"""

    print(f"íšŒì „ GIF ìƒì„± ì¤‘...")

    # ê·¸ë¦¼ ì„¤ì •
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')

    # í´ëŸ¬ìŠ¤í„°ë³„ ìƒ‰ìƒ
    unique_labels = np.unique(labels)
    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))

    # ì´ˆê¸° í”Œë¡¯
    scatters = []
    for i, label in enumerate(unique_labels):
        mask = labels == label
        scatter = ax.scatter(X_tsne[mask, 0], X_tsne[mask, 1], X_tsne[mask, 2],
                           c=[colors[i]], label=f'Cluster {label}', alpha=0.7, s=20)
        scatters.append(scatter)

    ax.set_xlabel('t-SNE 1')
    ax.set_ylabel('t-SNE 2')
    ax.set_zlabel('t-SNE 3')
    ax.set_title('t-SNE 3D Clustering (Rotating)')
    ax.legend()

    # ì¶• ë²”ìœ„ ê³ ì •
    ax.set_xlim(X_tsne[:, 0].min(), X_tsne[:, 0].max())
    ax.set_ylim(X_tsne[:, 1].min(), X_tsne[:, 1].max())
    ax.set_zlim(X_tsne[:, 2].min(), X_tsne[:, 2].max())

    def animate(frame):
        ax.view_init(elev=elevation, azim=frame * 4)  # 4ë„ì”© íšŒì „
        return scatters

    # ì• ë‹ˆë©”ì´ì…˜ ìƒì„± (90í”„ë ˆì„ = 360ë„)
    anim = animation.FuncAnimation(fig, animate, frames=90, interval=duration, blit=False)

    # GIF ì €ì¥
    anim.save(filename, writer='pillow', fps=10)
    plt.close()

    print(f"GIF ì €ì¥ ì™„ë£Œ: {filename}")

def create_high_quality_gif(X_tsne, labels, filename='tsne_3d_hq.gif',
                           frames=60, dpi=100):
    """ê³ í’ˆì§ˆ íšŒì „ GIF (PIL ì‚¬ìš©)"""

    print(f"ê³ í’ˆì§ˆ GIF ìƒì„± ì¤‘... ({frames}í”„ë ˆì„)")

    images = []

    # í´ëŸ¬ìŠ¤í„°ë³„ ìƒ‰ìƒ
    unique_labels = np.unique(labels)
    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))

    for frame in range(frames):
        fig = plt.figure(figsize=(10, 8), dpi=dpi)
        ax = fig.add_subplot(111, projection='3d')

        # í´ëŸ¬ìŠ¤í„°ë³„ í”Œë¡¯
        for i, label in enumerate(unique_labels):
            mask = labels == label
            ax.scatter(X_tsne[mask, 0], X_tsne[mask, 1], X_tsne[mask, 2],
                      c=[colors[i]], label=f'Cluster {label}', alpha=0.7, s=20)

        # ë·° ì„¤ì •
        ax.view_init(elev=20, azim=frame * 6)  # 6ë„ì”© íšŒì „
        ax.set_xlabel('t-SNE 1')
        ax.set_ylabel('t-SNE 2')
        ax.set_zlabel('t-SNE 3')
        ax.set_title('t-SNE 3D Clustering')

        if frame == 0:  # ì²« í”„ë ˆì„ì—ë§Œ ë²”ë¡€
            ax.legend()

        # ì´ë¯¸ì§€ë¡œ ë³€í™˜
        buf = io.BytesIO()
        plt.savefig(buf, format='png', bbox_inches='tight', dpi=dpi)
        buf.seek(0)
        img = Image.open(buf)
        images.append(img)

        plt.close()

        if frame % 10 == 0:
            print(f"ì§„í–‰ë¥ : {frame}/{frames}")

    # GIF ì €ì¥
    images[0].save(filename, save_all=True, append_images=images[1:],
                   duration=100, loop=0, optimize=True)

    print(f"ê³ í’ˆì§ˆ GIF ì €ì¥ ì™„ë£Œ: {filename}")

def analyze_tsne_clusters(X_tsne, labels, original_df):
    """t-SNE ê²°ê³¼ í´ëŸ¬ìŠ¤í„° ë¶„ì„"""

    print("\nt-SNE í´ëŸ¬ìŠ¤í„° ë¶„ì„:")
    print("-" * 40)

    # í´ëŸ¬ìŠ¤í„°ë³„ t-SNE ì¢Œí‘œ ì¤‘ì‹¬
    for label in np.unique(labels):
        mask = labels == label
        centroid = X_tsne[mask].mean(axis=0)
        size = np.sum(mask)
        print(f"í´ëŸ¬ìŠ¤í„° {label}: ì¤‘ì‹¬({centroid[0]:.2f}, {centroid[1]:.2f}, {centroid[2]:.2f}), í¬ê¸°={size}")

    # í´ëŸ¬ìŠ¤í„° ê°„ ê±°ë¦¬
    centroids = []
    for label in np.unique(labels):
        mask = labels == label
        centroid = X_tsne[mask].mean(axis=0)
        centroids.append(centroid)

    centroids = np.array(centroids)
    print(f"\ní´ëŸ¬ìŠ¤í„° ê°„ í‰ê·  ê±°ë¦¬: {np.mean(pdist(centroids)):.2f}")

def main_tsne_analysis(csv_file='cluster_results_kmeans.csv'):
    """ë©”ì¸ t-SNE ë¶„ì„ í•¨ìˆ˜"""

    # 1. ë°ì´í„° ë¡œë“œ
    X, cluster_labels, df = load_and_prepare_data(csv_file)

    # 2. t-SNE ì ìš©
    X_tsne, labels_sample, scaler, tsne = apply_tsne_3d(X, cluster_labels)

    # 3. ì •ì  ì‹œê°í™”
    plot_static_3d(X_tsne, labels_sample, "K-means Clusters - t-SNE 3D")

    # 4. íšŒì „ GIF ìƒì„± (ê¸°ë³¸)
    create_rotating_gif(X_tsne, labels_sample, 'kmeans_tsne_rotating.gif')

    # 5. ê³ í’ˆì§ˆ GIF ìƒì„±
    create_high_quality_gif(X_tsne, labels_sample, 'kmeans_tsne_hq.gif', frames=144)

    # 6. í´ëŸ¬ìŠ¤í„° ë¶„ì„
    analyze_tsne_clusters(X_tsne, labels_sample, df)

    return X_tsne, labels_sample, df

# ì‹¤í–‰
if __name__ == "__main__":
    X_tsne, labels, df = main_tsne_analysis('cluster_results_kmeans.csv')
    print("\nâœ… t-SNE 3D ì‹œê°í™” ë° GIF ìƒì„± ì™„ë£Œ!")

# ê°œë³„ ì‹¤í–‰ ì˜ˆì‹œ:
# X, cluster_labels, df = load_and_prepare_data('cluster_results_kmeans.csv')
# X_tsne, labels_sample, scaler, tsne = apply_tsne_3d(X, cluster_labels, perplexity=50)
# plot_static_3d(X_tsne, labels_sample)
# create_rotating_gif(X_tsne, labels_sample, 'my_rotation.gif')
# create_high_quality_gif(X_tsne, labels_sample, 'my_hq_rotation.gif', frames=90)
#%%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.patches import Polygon
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

def load_cluster_data(file_path='cluster_results_kmeans.csv'):
    """í´ëŸ¬ìŠ¤í„° ë°ì´í„° ë¡œë“œ"""
    df = pd.read_csv(file_path)
    print(f"ë°ì´í„° í¬ê¸°: {df.shape}")
    print(f"í´ëŸ¬ìŠ¤í„° ë¶„í¬: {dict(df['cluster'].value_counts().sort_index())}")
    return df

def get_analysis_features(df):
    """ë¶„ì„í•  í”¼ì²˜ ì„ íƒ"""
    # ìˆ˜ì¹˜í˜• í”¼ì²˜ ì„ íƒ (í…ìŠ¤íŠ¸, ì‹ë³„ì ì œì™¸)
    exclude_cols = ['ìƒí’ˆëª…', 'êµ¬ë§¤ìƒí’ˆëª…', 'í—¤ë“œë¼ì¸', 'ë¦¬ë·°ë‚´ìš©', 'full_text',
                   'êµ¬ë§¤ìëª…', 'ì‘ì„±ì¼ì', 'sentiment_label', 'cluster']

    numeric_cols = df.select_dtypes(include=[np.number]).columns
    feature_cols = [col for col in numeric_cols if col not in exclude_cols]

    return feature_cols

def create_cluster_heatmap(df, features):
    """í´ëŸ¬ìŠ¤í„°ë³„ í”¼ì²˜ í‰ê· ê°’ íˆíŠ¸ë§µ"""
    # í´ëŸ¬ìŠ¤í„°ë³„ í‰ê·  ê³„ì‚°
    cluster_stats = df.groupby('cluster')[features].mean()

    # ì •ê·œí™” (0-1 ìŠ¤ì¼€ì¼)
    cluster_stats_norm = (cluster_stats - cluster_stats.min()) / (cluster_stats.max() - cluster_stats.min())

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))

    # ì›ë³¸ ê°’ íˆíŠ¸ë§µ
    sns.heatmap(cluster_stats, annot=True, fmt='.2f', cmap='RdYlBu_r',
                ax=ax1, cbar_kws={'label': 'Average Value'})
    ax1.set_title('Cluster Feature Averages (Original Values)')
    ax1.set_xlabel('Features')
    ax1.set_ylabel('Cluster')

    # ì •ê·œí™”ëœ ê°’ íˆíŠ¸ë§µ
    sns.heatmap(cluster_stats_norm, annot=True, fmt='.2f', cmap='RdYlBu_r',
                ax=ax2, cbar_kws={'label': 'Normalized Value (0-1)'})
    ax2.set_title('Cluster Feature Averages (Normalized)')
    ax2.set_xlabel('Features')
    ax2.set_ylabel('Cluster')

    plt.tight_layout()
    plt.show()

    return cluster_stats

def create_radar_charts(df, features):
    """í´ëŸ¬ìŠ¤í„°ë³„ ë ˆì´ë” ì°¨íŠ¸"""
    cluster_stats = df.groupby('cluster')[features].mean()

    # ì •ê·œí™”
    cluster_stats_norm = (cluster_stats - cluster_stats.min()) / (cluster_stats.max() - cluster_stats.min())

    n_clusters = len(cluster_stats)
    n_features = len(features)

    # ê°ë„ ê³„ì‚°
    angles = np.linspace(0, 2 * np.pi, n_features, endpoint=False).tolist()
    angles += angles[:1]  # ì›í˜•ìœ¼ë¡œ ë‹«ê¸°

    # 2x4 ë°°ì¹˜
    fig, axes = plt.subplots(2, 4, figsize=(20, 10), subplot_kw=dict(projection='polar'))
    axes = axes.flatten()

    colors = plt.cm.Set3(np.linspace(0, 1, n_clusters))

    for i, cluster in enumerate(cluster_stats_norm.index):
        ax = axes[i]
        values = cluster_stats_norm.loc[cluster].tolist()
        values += values[:1]  # ì›í˜•ìœ¼ë¡œ ë‹«ê¸°

        ax.plot(angles, values, 'o-', linewidth=2, label=f'Cluster {cluster}', color=colors[i])
        ax.fill(angles, values, alpha=0.25, color=colors[i])
        ax.set_ylim(0, 1)
        ax.set_title(f'Cluster {cluster}', size=14, weight='bold')
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels([feat[:10] for feat in features], size=8)
        ax.grid(True)

    plt.tight_layout()
    plt.show()

def create_feature_comparison(df, features):
    """ì£¼ìš” í”¼ì²˜ë³„ í´ëŸ¬ìŠ¤í„° ë¹„êµ (ë°•ìŠ¤í”Œë¡¯)"""
    # ì£¼ìš” í”¼ì²˜ ì„ íƒ (ë¶„ì‚°ì´ í° ê²ƒë“¤)
    feature_vars = df[features].var().sort_values(ascending=False)
    top_features = feature_vars.head(8).index.tolist()

    fig, axes = plt.subplots(2, 4, figsize=(20, 10))
    axes = axes.flatten()

    for i, feature in enumerate(top_features):
        ax = axes[i]
        df.boxplot(column=feature, by='cluster', ax=ax)
        ax.set_title(f'{feature}')
        ax.set_xlabel('Cluster')
        ax.set_ylabel('Value')

    plt.suptitle('Feature Distribution by Cluster (Top 8 Most Variable Features)', size=16)
    plt.tight_layout()
    plt.show()

def create_cluster_summary_table(df, features):
    """í´ëŸ¬ìŠ¤í„°ë³„ ìš”ì•½ í†µê³„ í…Œì´ë¸”"""
    summary_data = []

    for cluster in sorted(df['cluster'].unique()):
        cluster_data = df[df['cluster'] == cluster]

        summary = {
            'Cluster': cluster,
            'Size': len(cluster_data),
            'Percentage': f"{len(cluster_data)/len(df)*100:.1f}%"
        }

        # ì£¼ìš” í”¼ì²˜ í‰ê· ê°’
        key_features = ['í‰ì ', 'full_text_length', 'emoticon_count', 'emphasis_count',
                       'sentiment_positive', 'sentiment_negative', 'recommend_mention']

        for feat in key_features:
            if feat in cluster_data.columns:
                summary[feat] = f"{cluster_data[feat].mean():.2f}"

        summary_data.append(summary)

    summary_df = pd.DataFrame(summary_data)

    # í…Œì´ë¸” ì‹œê°í™”
    fig, ax = plt.subplots(figsize=(15, 6))
    ax.axis('tight')
    ax.axis('off')

    table = ax.table(cellText=summary_df.values,
                    colLabels=summary_df.columns,
                    cellLoc='center',
                    loc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1.2, 1.5)

    # í—¤ë” ìŠ¤íƒ€ì¼
    for i in range(len(summary_df.columns)):
        table[(0, i)].set_facecolor('#4CAF50')
        table[(0, i)].set_text_props(weight='bold', color='white')

    plt.title('Cluster Summary Statistics', size=16, weight='bold', pad=20)
    plt.show()

    return summary_df

def create_parallel_coordinates(df, features):
    """ë³‘ë ¬ ì¢Œí‘œ í”Œë¡¯"""
    # ì£¼ìš” í”¼ì²˜ë§Œ ì„ íƒ (ë„ˆë¬´ ë§ìœ¼ë©´ ë³µì¡í•¨)
    selected_features = features[:8] if len(features) > 8 else features

    # ë°ì´í„° ì •ê·œí™”
    plot_data = df[selected_features + ['cluster']].copy()
    for feat in selected_features:
        plot_data[feat] = (plot_data[feat] - plot_data[feat].min()) / (plot_data[feat].max() - plot_data[feat].min())

    # í´ëŸ¬ìŠ¤í„°ë³„ í‰ê·  ê³„ì‚°
    cluster_means = plot_data.groupby('cluster')[selected_features].mean()

    fig, ax = plt.subplots(figsize=(15, 8))

    colors = plt.cm.Set3(np.linspace(0, 1, len(cluster_means)))

    for i, (cluster, values) in enumerate(cluster_means.iterrows()):
        ax.plot(range(len(selected_features)), values, 'o-',
                linewidth=3, markersize=8, label=f'Cluster {cluster}',
                color=colors[i], alpha=0.8)

    ax.set_xticks(range(len(selected_features)))
    ax.set_xticklabels([feat[:15] for feat in selected_features], rotation=45, ha='right')
    ax.set_ylabel('Normalized Value')
    ax.set_title('Parallel Coordinates Plot - Cluster Comparison', size=16, weight='bold')
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

def analyze_cluster_characteristics(df):
    """í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ë¶„ì„ ë° í•´ì„"""
    print("="*60)
    print("í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ë¶„ì„")
    print("="*60)

    for cluster in sorted(df['cluster'].unique()):
        cluster_data = df[df['cluster'] == cluster]
        print(f"\nğŸ” í´ëŸ¬ìŠ¤í„° {cluster} (n={len(cluster_data)}, {len(cluster_data)/len(df)*100:.1f}%)")
        print("-" * 40)

        # í‰ì  íŠ¹ì„±
        if 'í‰ì ' in cluster_data.columns:
            avg_rating = cluster_data['í‰ì '].mean()
            rating_dist = cluster_data['í‰ì '].value_counts().sort_index()
            print(f"í‰ì : í‰ê·  {avg_rating:.2f}, ë¶„í¬ {dict(rating_dist)}")

        # ê°ì • íŠ¹ì„±
        sentiment_cols = ['sentiment_positive', 'sentiment_negative', 'sentiment_neutral']
        if all(col in cluster_data.columns for col in sentiment_cols):
            pos_rate = cluster_data['sentiment_positive'].mean()
            neg_rate = cluster_data['sentiment_negative'].mean()
            neu_rate = cluster_data['sentiment_neutral'].mean()
            print(f"ê°ì •: ê¸ì • {pos_rate:.1%}, ë¶€ì • {neg_rate:.1%}, ì¤‘ë¦½ {neu_rate:.1%}")

        # í–‰ë™ íŠ¹ì„±
        behavior_cols = ['recommend_mention', 'repurchase_mention', 'delivery_mention']
        behavior_rates = {}
        for col in behavior_cols:
            if col in cluster_data.columns:
                behavior_rates[col.replace('_mention', '')] = cluster_data[col].mean()

        if behavior_rates:
            print(f"í–‰ë™: " + ", ".join([f"{k} {v:.1%}" for k, v in behavior_rates.items()]))

        # í…ìŠ¤íŠ¸ íŠ¹ì„±
        if 'full_text_length' in cluster_data.columns:
            avg_length = cluster_data['full_text_length'].mean()
            print(f"í…ìŠ¤íŠ¸: í‰ê·  {avg_length:.0f}ì")

        if 'emphasis_count' in cluster_data.columns:
            avg_emphasis = cluster_data['emphasis_count'].mean()
            print(f"ê°•ì¡°í‘œí˜„: í‰ê·  {avg_emphasis:.1f}ê°œ")

def main_cluster_analysis(file_path='cluster_results_kmeans.csv'):
    """ë©”ì¸ í´ëŸ¬ìŠ¤í„° ë¶„ì„ í•¨ìˆ˜"""

    print("ğŸ” í´ëŸ¬ìŠ¤í„°ë³„ í”¼ì²˜ ë¶„ì„ ì‹œì‘")
    print("="*50)

    # 1. ë°ì´í„° ë¡œë“œ
    df = load_cluster_data(file_path)

    # 2. ë¶„ì„ í”¼ì²˜ ì„ íƒ
    features = get_analysis_features(df)
    print(f"\në¶„ì„ í”¼ì²˜ ({len(features)}ê°œ): {features}")

    # 3. íˆíŠ¸ë§µ ì‹œê°í™”
    print("\nğŸ“Š 1. í´ëŸ¬ìŠ¤í„°ë³„ í”¼ì²˜ í‰ê·  íˆíŠ¸ë§µ")
    cluster_stats = create_cluster_heatmap(df, features)

    # 4. ë ˆì´ë” ì°¨íŠ¸
    print("\nğŸ¯ 2. í´ëŸ¬ìŠ¤í„°ë³„ ë ˆì´ë” ì°¨íŠ¸")
    create_radar_charts(df, features)

    # 5. ë°•ìŠ¤í”Œë¡¯ ë¹„êµ
    print("\nğŸ“ˆ 3. ì£¼ìš” í”¼ì²˜ ë¶„í¬ ë¹„êµ")
    create_feature_comparison(df, features)

    # 6. ìš”ì•½ í…Œì´ë¸”
    print("\nğŸ“‹ 4. í´ëŸ¬ìŠ¤í„° ìš”ì•½ í†µê³„")
    summary_df = create_cluster_summary_table(df, features)

    # 7. ë³‘ë ¬ ì¢Œí‘œ í”Œë¡¯
    print("\nğŸ”— 5. ë³‘ë ¬ ì¢Œí‘œ í”Œë¡¯")
    create_parallel_coordinates(df, features)

    # 8. íŠ¹ì„± ë¶„ì„
    print("\nğŸ’¡ 6. í´ëŸ¬ìŠ¤í„° íŠ¹ì„± í•´ì„")
    analyze_cluster_characteristics(df)

    return cluster_stats, summary_df

# ì‹¤í–‰
cluster_stats, summary_df = main_cluster_analysis('cluster_results_kmeans.csv')