from bs4 import BeautifulSoup as bs
from openpyxl import Workbook
from fake_useragent import UserAgent
from requests.exceptions import RequestException, Timeout, ConnectTimeout, ReadTimeout
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import time
import os
import re
import requests as rq
import random
import json


class NonWindowsUserAgent:
    """Windowsë¥¼ ì œì™¸í•œ User-Agent ìƒì„±ê¸° (fake_useragent ê¸°ë°˜)"""

    def __init__(self):
        self.ua = UserAgent()
        self.max_attempts = 20  # ìµœëŒ€ ì‹œë„ íšŸìˆ˜

        # Windows ê´€ë ¨ í‚¤ì›Œë“œë“¤
        self.windows_keywords = [
            'Windows NT', 'Win32', 'Win64', 'WOW64', 'Windows 10', 'Windows 11',
            'Windows 7', 'Windows 8', 'Microsoft Windows', 'win32', 'win64'
        ]

    def _is_windows_ua(self, user_agent):
        """User-Agentê°€ Windowsì¸ì§€ í™•ì¸"""
        if not user_agent:
            return True

        user_agent_lower = user_agent.lower()
        return any(keyword.lower() in user_agent_lower for keyword in self.windows_keywords)

    def _get_non_windows_ua(self, ua_type='random'):
        """Windowsê°€ ì•„ë‹Œ User-Agentë¥¼ ê°€ì ¸ì˜¤ê¸°"""
        for attempt in range(self.max_attempts):
            try:
                if ua_type == 'chrome':
                    user_agent = self.ua.chrome
                elif ua_type == 'firefox':
                    user_agent = self.ua.firefox
                elif ua_type == 'safari':
                    user_agent = self.ua.safari
                else:
                    user_agent = self.ua.random

                if not self._is_windows_ua(user_agent):
                    print(f"[DEBUG] User-Agent ì„ íƒ ì„±ê³µ (ì‹œë„ {attempt + 1}íšŒ): {user_agent[:50]}...")
                    return user_agent
                else:
                    print(f"[DEBUG] Windows UA ê°ì§€, ì¬ì‹œë„ ì¤‘... (ì‹œë„ {attempt + 1}/{self.max_attempts})")

            except Exception as e:
                print(f"[WARNING] User-Agent ìƒì„± ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {e}")
                continue

        # ëª¨ë“  ì‹œë„ê°€ ì‹¤íŒ¨í•œ ê²½ìš° ì•ˆì „í•œ Mac UA ë°˜í™˜
        print(f"[WARNING] {self.max_attempts}íšŒ ì‹œë„ í›„ì—ë„ ì ì ˆí•œ UAë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ Mac UA ì‚¬ìš©")
        return "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

    @property
    def random(self):
        """ëœë¤í•œ Non-Windows User-Agent ë°˜í™˜"""
        return self._get_non_windows_ua('random')

    @property
    def chrome(self):
        """Chrome Non-Windows User-Agent ë°˜í™˜"""
        return self._get_non_windows_ua('chrome')

    @property
    def firefox(self):
        """Firefox Non-Windows User-Agent ë°˜í™˜"""
        return self._get_non_windows_ua('firefox')

    @property
    def safari(self):
        """Safari Non-Windows User-Agent ë°˜í™˜"""
        return self._get_non_windows_ua('safari')

    def get_mobile_ua(self):
        """ëª¨ë°”ì¼ ì „ìš© User-Agent ë°˜í™˜"""
        for attempt in range(self.max_attempts):
            try:
                # ëª¨ë°”ì¼ ë¸Œë¼ìš°ì € ìœ„ì£¼ë¡œ ì‹œë„
                browser_types = ['chrome', 'safari', 'random']
                ua_type = random.choice(browser_types)

                user_agent = self._get_non_windows_ua(ua_type)

                # ëª¨ë°”ì¼ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                mobile_keywords = ['Mobile', 'Android', 'iPhone', 'iPad']
                if any(keyword in user_agent for keyword in mobile_keywords):
                    return user_agent

            except Exception as e:
                print(f"[WARNING] ëª¨ë°”ì¼ UA ìƒì„± ì˜¤ë¥˜: {e}")
                continue

        # ì‹¤íŒ¨ ì‹œ ì•ˆì „í•œ Android UA ë°˜í™˜
        return "Mozilla/5.0 (Linux; Android 10; SM-G973F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36"

    def get_desktop_ua(self):
        """ë°ìŠ¤í¬í†± ì „ìš© User-Agent ë°˜í™˜ (Mac ìœ„ì£¼)"""
        for attempt in range(self.max_attempts):
            try:
                user_agent = self._get_non_windows_ua('random')

                # ë°ìŠ¤í¬í†±ì´ë©´ì„œ ëª¨ë°”ì¼ì´ ì•„ë‹Œ ê²ƒ
                if 'Macintosh' in user_agent and 'Mobile' not in user_agent:
                    return user_agent

            except Exception as e:
                print(f"[WARNING] ë°ìŠ¤í¬í†± UA ìƒì„± ì˜¤ë¥˜: {e}")
                continue

        # ì‹¤íŒ¨ ì‹œ ì•ˆì „í•œ Mac UA ë°˜í™˜
        return "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15"


class ProxyRotator:
    def __init__(self, proxy_list=None):
        """
        í”„ë¡ì‹œ ë¡œí…Œì´í„° ì´ˆê¸°í™”
        proxy_list: ['ip:port:username:password', ...] í˜•íƒœì˜ í”„ë¡ì‹œ ë¦¬ìŠ¤íŠ¸
        """
        self.proxy_list = proxy_list if proxy_list else []
        # itertools.cycle ì œê±° - ëœë¤ ì„ íƒìœ¼ë¡œ ë³€ê²½
        self.current_proxy = None
        self.failed_proxies = set()
        self.proxy_failure_count = {}  # í”„ë¡ì‹œë³„ ì‹¤íŒ¨ íšŸìˆ˜ ì¶”ì 
        self.max_failures_per_proxy = 3  # í”„ë¡ì‹œë‹¹ ìµœëŒ€ ì‹¤íŒ¨ í—ˆìš© íšŸìˆ˜

    def get_next_proxy(self):
        """ëœë¤í•˜ê²Œ í”„ë¡ì‹œë¥¼ ì„ íƒí•˜ì—¬ ë°˜í™˜"""
        if not self.proxy_list:
            return None

        # ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œ ëª©ë¡ ìƒì„±
        available_proxies = [proxy for proxy in self.proxy_list if proxy not in self.failed_proxies]

        if not available_proxies:
            # ëª¨ë“  í”„ë¡ì‹œê°€ ì™„ì „íˆ ì‹¤íŒ¨í–ˆë‹¤ë©´ ì‹¤íŒ¨ ëª©ë¡ì„ ì´ˆê¸°í™”
            print("[WARNING] ëª¨ë“  í”„ë¡ì‹œê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì‹¤íŒ¨ ëª©ë¡ê³¼ ì¹´ìš´í„°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
            self.failed_proxies.clear()
            self.proxy_failure_count.clear()
            available_proxies = self.proxy_list.copy()

        # ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œ ì¤‘ì—ì„œ ëœë¤í•˜ê²Œ ì„ íƒ
        proxy = random.choice(available_proxies)
        self.current_proxy = proxy

        proxy_ip = proxy.split(':')[0]
        failure_count = self.proxy_failure_count.get(proxy, 0)
        print(f"[PROXY] ëœë¤ ì„ íƒëœ í”„ë¡ì‹œ: {proxy_ip} (ì‹¤íŒ¨ íšŸìˆ˜: {failure_count})")
        return proxy

    def get_random_proxy_from_working_set(self):
        """ì„±ëŠ¥ì´ ì¢‹ì€ í”„ë¡ì‹œë“¤ ì¤‘ì—ì„œ ëœë¤ ì„ íƒ"""
        if not self.proxy_list:
            return None

        # ì‹¤íŒ¨ íšŸìˆ˜ê°€ ì ì€ í”„ë¡ì‹œë“¤ì„ ìš°ì„ ì ìœ¼ë¡œ ì„ íƒ
        working_proxies = []
        for proxy in self.proxy_list:
            if proxy not in self.failed_proxies:
                failure_count = self.proxy_failure_count.get(proxy, 0)
                # ì‹¤íŒ¨ íšŸìˆ˜ê°€ 1íšŒ ì´í•˜ì¸ í”„ë¡ì‹œë“¤ì„ ìš°ì„  ì„ íƒ
                if failure_count <= 1:
                    working_proxies.append(proxy)

        # ì„±ëŠ¥ ì¢‹ì€ í”„ë¡ì‹œê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ì„ íƒ ë°©ì‹ ì‚¬ìš©
        if not working_proxies:
            return self.get_next_proxy()

        proxy = random.choice(working_proxies)
        self.current_proxy = proxy
        proxy_ip = proxy.split(':')[0]
        failure_count = self.proxy_failure_count.get(proxy, 0)
        print(f"[PROXY] ì„±ëŠ¥ ìš°ì„  ëœë¤ ì„ íƒ: {proxy_ip} (ì‹¤íŒ¨ íšŸìˆ˜: {failure_count})")
        return proxy

    def mark_proxy_failed(self, proxy):
        """í”„ë¡ì‹œë¥¼ ì‹¤íŒ¨ë¡œ í‘œì‹œ (ëˆ„ì  ì‹¤íŒ¨ ê´€ë¦¬)"""
        if proxy not in self.proxy_failure_count:
            self.proxy_failure_count[proxy] = 0

        self.proxy_failure_count[proxy] += 1
        proxy_ip = proxy.split(':')[0]

        # ìµœëŒ€ ì‹¤íŒ¨ íšŸìˆ˜ì— ë„ë‹¬í•˜ë©´ ì™„ì „íˆ ì œê±°
        if self.proxy_failure_count[proxy] >= self.max_failures_per_proxy:
            self.failed_proxies.add(proxy)
            print(f"[WARNING] í”„ë¡ì‹œ ì™„ì „ ì‹¤íŒ¨ë¡œ ì œê±°: {proxy_ip} ({self.proxy_failure_count[proxy]}íšŒ ì‹¤íŒ¨)")
        else:
            print(
                f"[WARNING] í”„ë¡ì‹œ ì¼ì‹œ ì‹¤íŒ¨: {proxy_ip} ({self.proxy_failure_count[proxy]}/{self.max_failures_per_proxy} ì‹¤íŒ¨)")

    def get_available_proxy_count(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œ ê°œìˆ˜ ë°˜í™˜"""
        if not self.proxy_list:
            return 0
        return len(self.proxy_list) - len(self.failed_proxies)

    def get_proxy_dict(self, proxy_string):
        """í”„ë¡ì‹œ ë¬¸ìì—´ì„ requestsìš© ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        if not proxy_string:
            return None

        parts = proxy_string.split(':')
        if len(parts) == 2:  # ip:port
            ip, port = parts
            return {
                'http': f'http://{ip}:{port}',
                'https': f'http://{ip}:{port}'
            }
        elif len(parts) == 4:  # ip:port:username:password
            ip, port, username, password = parts
            return {
                'http': f'http://{username}:{password}@{ip}:{port}',
                'https': f'http://{username}:{password}@{ip}:{port}'
            }
        return None


class ChromeDriver:
    def __init__(self, proxy_rotator=None) -> None:
        self.proxy_rotator = proxy_rotator
        self.ua = NonWindowsUserAgent()  # Windows ì œì™¸ User-Agent ì‚¬ìš©
        self.set_options()
        self.set_driver()

    def set_options(self) -> None:
        self.options = Options()
        self.options.add_argument("--headless")
        self.options.add_argument("lang=ko_KR")

        # ëª¨ë°”ì¼/Mac ì „ìš© User-Agent ì‚¬ìš©
        user_agent = self.ua.random
        self.options.add_argument(f"user-agent={user_agent}")
        print(f"[DEBUG] ì‚¬ìš© ì¤‘ì¸ User-Agent: {user_agent}")

        # ë” ë§ì€ ë¸Œë¼ìš°ì € ì˜µì…˜ ì¶”ê°€ë¡œ íƒì§€ ë°©ì§€
        self.options.add_argument("--log-level=3")
        self.options.add_argument("--no-sandbox")
        self.options.add_argument("--disable-dev-shm-usage")
        self.options.add_argument("--disable-blink-features=AutomationControlled")
        self.options.add_argument("--exclude-switches=enable-automation")
        self.options.add_argument("--disable-extensions")
        self.options.add_argument("--no-first-run")
        self.options.add_argument("--disable-default-apps")
        self.options.add_argument("--disable-infobars")
        self.options.add_experimental_option("detach", True)
        self.options.add_experimental_option("excludeSwitches", ["enable-automation"])
        self.options.add_experimental_option('useAutomationExtension', False)

        # í”„ë¡ì‹œ ì„¤ì •
        if self.proxy_rotator:
            proxy = self.proxy_rotator.get_next_proxy()
            if proxy:
                parts = proxy.split(':')
                if len(parts) >= 2:
                    ip, port = parts[0], parts[1]
                    self.options.add_argument(f'--proxy-server=http://{ip}:{port}')
                    print(f"[DEBUG] Selenium í”„ë¡ì‹œ ì„¤ì •: {ip}:{port}")

    def set_driver(self) -> None:
        self.driver = webdriver.Chrome(options=self.options)
        # WebDriver íƒì§€ ë°©ì§€
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

    def refresh_with_new_proxy(self):
        """ìƒˆë¡œìš´ í”„ë¡ì‹œë¡œ ë“œë¼ì´ë²„ ì¬ì‹œì‘"""
        if hasattr(self, 'driver') and self.driver:
            self.driver.quit()
        self.set_options()
        self.set_driver()


class URLManager:
    """URL ê´€ë¦¬ í´ë˜ìŠ¤ (JSON ì§€ì›)"""

    def __init__(self, file_path="data/í™ˆí”Œë˜ë‹›_products_dedup_first.json"):
        self.file_path = file_path
        self.products = []  # URLê³¼ ìƒí’ˆëª…ì„ í•¨ê»˜ ì €ì¥
        self.current_index = 0

    def load_urls_from_json(self):
        """JSON íŒŒì¼ì—ì„œ URL ëª©ë¡ê³¼ ìƒí’ˆëª… ë¡œë“œ"""
        try:
            if not os.path.exists(self.file_path):
                self.create_sample_file()
                return False

            with open(self.file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            self.products = []

            # JSONì´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
            if isinstance(data, list):
                for i, item in enumerate(data, 1):
                    if isinstance(item, dict) and 'product_url' in item:
                        url = item['product_url']
                        product_name = item.get('product_name', f'ìƒí’ˆ_{i}')

                        if "coupang.com" in url and "products/" in url:
                            self.products.append({
                                'url': url,
                                'name': product_name
                            })
                        else:
                            print(f"[WARNING] ì˜ëª»ëœ URL í˜•ì‹ (í•­ëª© {i}): {url}")

            if self.products:
                print(f"[INFO] {len(self.products)}ê°œì˜ ìœ íš¨í•œ ìƒí’ˆì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                return True
            else:
                print("[ERROR] ìœ íš¨í•œ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
                return False

        except json.JSONDecodeError as e:
            print(f"[ERROR] JSON íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return False
        except Exception as e:
            print(f"[ERROR] JSON íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            return False

    def create_sample_file(self):
        """ìƒ˜í”Œ JSON íŒŒì¼ ìƒì„±"""
        sample_data = [
            {
                "product_url": "https://www.coupang.com/vp/products/7335597976?itemId=18741704367&vendorItemId=85873964906",
                "product_name": "ìƒ˜í”Œ ìƒí’ˆ 1"
            },
            {
                "product_url": "https://www.coupang.com/vp/products/1234567890?itemId=12345678901&vendorItemId=98765432109",
                "product_name": "ìƒ˜í”Œ ìƒí’ˆ 2"
            }
        ]

        try:
            with open(self.file_path, 'w', encoding='utf-8') as f:
                json.dump(sample_data, f, ensure_ascii=False, indent=2)
            print(f"[INFO] ìƒ˜í”Œ JSON íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {self.file_path}")
            print("[INFO] íŒŒì¼ì„ í¸ì§‘í•˜ì—¬ í¬ë¡¤ë§í•  ìƒí’ˆ ì •ë³´ë¥¼ ì…ë ¥í•œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
        except Exception as e:
            print(f"[ERROR] ìƒ˜í”Œ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")

    def get_next_product(self):
        """ë‹¤ìŒ ìƒí’ˆ ì •ë³´ ë°˜í™˜ (URLê³¼ ìƒí’ˆëª…)"""
        if self.current_index < len(self.products):
            product = self.products[self.current_index]
            self.current_index += 1
            return product
        return None

    def get_remaining_count(self):
        """ë‚¨ì€ ìƒí’ˆ ê°œìˆ˜ ë°˜í™˜"""
        return len(self.products) - self.current_index

    def get_current_progress(self):
        """í˜„ì¬ ì§„í–‰ë¥  ë°˜í™˜"""
        return self.current_index, len(self.products)


class Coupang:
    @staticmethod
    def get_product_code(url: str) -> str:
        prod_code: str = url.split("products/")[-1].split("?")[0]
        return prod_code

    @staticmethod
    def get_soup_object(resp: rq.Response) -> bs:
        return bs(resp.text, "html.parser")

    def __del__(self) -> None:
        if hasattr(self, 'ch') and self.ch.driver:
            self.ch.driver.quit()

    def __init__(self, proxy_list=None) -> None:
        # delay ê´€ë ¨ ì„¤ì •
        self.base_review_url: str = "https://www.coupang.com/vp/product/reviews"
        self.retries = 10  # ì¬ì‹œë„ íšŸìˆ˜ ì¤„ì„
        self.delay_min = 1.0  # ìµœì†Œ ë”œë ˆì´ ì¦ê°€
        self.delay_max = 2.0  # ìµœëŒ€ ë”œë ˆì´ ì¦ê°€
        self.page_delay_min = 0.0  # í˜ì´ì§€ ê°„ ìµœì†Œ ë”œë ˆì´ ì¦ê°€
        self.page_delay_max = 0.0  # í˜ì´ì§€ ê°„ ìµœëŒ€ ë”œë ˆì´ ì¦ê°€
        self.max_pages = 150  # v1.6: ìµœëŒ€ í˜ì´ì§€ë¥¼ 300ìœ¼ë¡œ ì œí•œ

        # íƒ€ì„ì•„ì›ƒ ê´€ë ¨ ì„¤ì •
        self.consecutive_timeouts = 0
        self.max_consecutive_timeouts = 5  # ì—°ì† íƒ€ì„ì•„ì›ƒ í—ˆìš© íšŸìˆ˜ ê°ì†Œ
        self.long_wait_min = 10  # ê¸´ ëŒ€ê¸° ì‹œê°„ ì¤„ì„ (5ë¶„)
        self.long_wait_max = 15  # ê¸´ ëŒ€ê¸° ì‹œê°„ ì¤„ì„ (7ë¶„)

        # í”„ë¡ì‹œ ë¡œí…Œì´í„° ì´ˆê¸°í™”
        self.proxy_rotator = ProxyRotator(proxy_list)

        # Windows ì œì™¸ User-Agent ì´ˆê¸°í™”
        self.ua = NonWindowsUserAgent()

        # ë” ì •êµí•œ í—¤ë” ì„¤ì •
        self.base_headers = {
            "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "accept-encoding": "gzip, deflate, br, zstd",
            "accept-language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "cache-control": "max-age=0",
            "sec-ch-ua": '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"macOS"',  # Windows ëŒ€ì‹  macOS ì‚¬ìš©
            "sec-fetch-dest": "document",
            "sec-fetch-mode": "navigate",
            "sec-fetch-site": "same-origin",
            "sec-fetch-user": "?1",
            "upgrade-insecure-requests": "1",
            "dnt": "1",
        }

        # ì¿ í‚¤ ì €ì¥ìš© ì„¸ì…˜
        self.session = rq.Session()

        # í—¤ë”ì— ëœë¤ User-Agent ì ìš©
        self.update_headers()

        self.ch = ChromeDriver(self.proxy_rotator)
        self.page_title = None

        # v1.6: URL ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.url_manager = URLManager()

    def get_realistic_headers(self):
        """ì‹¤ì œ ë¸Œë¼ìš°ì €ì™€ ìœ ì‚¬í•œ í—¤ë” ìƒì„± (Windows ì œì™¸)"""
        headers = self.base_headers.copy()
        user_agent = self.ua.random
        headers["user-agent"] = user_agent

        # User-Agentì— ë”°ë¼ í”Œë«í¼ ì •ë³´ ì¡°ì •
        if 'iPhone' in user_agent or 'iPad' in user_agent:
            headers["sec-ch-ua-platform"] = '"iOS"'
            headers["sec-ch-ua-mobile"] = "?1" if 'iPhone' in user_agent else "?0"
        elif 'Android' in user_agent:
            headers["sec-ch-ua-platform"] = '"Android"'
            headers["sec-ch-ua-mobile"] = "?1"
        elif 'Macintosh' in user_agent or 'Mac OS X' in user_agent:
            headers["sec-ch-ua-platform"] = '"macOS"'
            headers["sec-ch-ua-mobile"] = "?0"
        elif 'Linux' in user_agent and 'Android' not in user_agent:
            headers["sec-ch-ua-platform"] = '"Linux"'
            headers["sec-ch-ua-mobile"] = "?0"
        else:
            # ê¸°ë³¸ê°’ì€ macOSë¡œ ì„¤ì • (Windows ë°©ì§€)
            headers["sec-ch-ua-platform"] = '"macOS"'
            headers["sec-ch-ua-mobile"] = "?0"

        # ëœë¤ ìš”ì†Œ ì¶”ê°€
        if random.choice([True, False]):
            headers["x-requested-with"] = "XMLHttpRequest"

        # ì¿ íŒ¡ íŠ¹í™” í—¤ë”
        headers.update({
            "x-coupang-target-market": "KR",
            "x-coupang-accept-language": "ko-KR",
        })

        return headers

    def update_headers(self):
        """í—¤ë”ë¥¼ ìƒˆë¡œìš´ User-Agentë¡œ ì—…ë°ì´íŠ¸"""
        self.headers = self.get_realistic_headers()
        print(f"[DEBUG] í—¤ë” User-Agent ì—…ë°ì´íŠ¸: {self.headers['user-agent'][:70]}...")

    def get_session_with_proxy(self):
        """í”„ë¡ì‹œê°€ ì ìš©ëœ requests ì„¸ì…˜ ë°˜í™˜"""
        session = rq.Session()
        session.headers.update(self.headers)

        # ë” í˜„ì‹¤ì ì¸ íƒ€ì„ì•„ì›ƒ ì„¤ì •
        session.timeout = (10, 30)  # ì—°ê²° íƒ€ì„ì•„ì›ƒ 10ì´ˆ, ì½ê¸° íƒ€ì„ì•„ì›ƒ 30ì´ˆ

        if self.proxy_rotator and self.proxy_rotator.proxy_list:
            # ì„±ëŠ¥ ìš°ì„  ëœë¤ ì„ íƒ ì‚¬ìš©
            proxy = self.proxy_rotator.get_random_proxy_from_working_set()
            if proxy:
                proxy_dict = self.proxy_rotator.get_proxy_dict(proxy)
                if proxy_dict:
                    session.proxies.update(proxy_dict)
                    print(f"[DEBUG] ìš”ì²­ì— í”„ë¡ì‹œ ì ìš©: {proxy}")

        return session

    def warm_up_session(self, prod_code):
        """ì„¸ì…˜ì„ ì˜ˆì—´í•˜ì—¬ ì¿ íŒ¡ ì‚¬ì´íŠ¸ì™€ì˜ ì—°ê²°ì„ ì„¤ì •"""
        try:
            print("[INFO] ì„¸ì…˜ ì˜ˆì—´ ì¤‘...")

            # ë©”ì¸ í˜ì´ì§€ ë¨¼ì € ë°©ë¬¸
            main_url = "https://www.coupang.com"
            session = self.get_session_with_proxy()

            # ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸
            resp = session.get(main_url, timeout=15)
            if resp.status_code == 200:
                print("[DEBUG] ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸ ì„±ê³µ")

                # ì¿ í‚¤ ì—…ë°ì´íŠ¸
                self.session.cookies.update(resp.cookies)

                # ì ì‹œ ëŒ€ê¸°
                time.sleep(random.uniform(2, 4))

                # ìƒí’ˆ í˜ì´ì§€ ë°©ë¬¸
                product_url = f"https://www.coupang.com/vp/products/{prod_code}"
                resp2 = session.get(product_url, timeout=15)

                if resp2.status_code == 200:
                    print("[DEBUG] ìƒí’ˆ í˜ì´ì§€ ë°©ë¬¸ ì„±ê³µ")
                    self.session.cookies.update(resp2.cookies)
                    return True

        except Exception as e:
            print(f"[WARNING] ì„¸ì…˜ ì˜ˆì—´ ì‹¤íŒ¨: {e}")

        return False

    def get_product_title(self, product_name: str) -> str:
        """JSONì—ì„œ ê°€ì ¸ì˜¨ ìƒí’ˆëª… ì‚¬ìš©"""
        print(f"[DEBUG] JSONì—ì„œ ê°€ì ¸ì˜¨ ìƒí’ˆëª… ì‚¬ìš©: {product_name}")
        return product_name

    def is_timeout_error(self, exception) -> bool:
        """íƒ€ì„ì•„ì›ƒ ê´€ë ¨ ì˜ˆì™¸ì¸ì§€ í™•ì¸"""
        return isinstance(exception, (Timeout, ConnectTimeout, ReadTimeout)) or \
            (isinstance(exception, RequestException) and "timeout" in str(exception).lower())

    def handle_consecutive_timeouts(self) -> None:
        """ì—°ì† íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬"""
        if self.consecutive_timeouts >= self.max_consecutive_timeouts:
            wait_time = random.uniform(self.long_wait_min, self.long_wait_max)
            wait_minutes = wait_time / 60
            print(f"[WARNING] ì—°ì† {self.consecutive_timeouts}íšŒ íƒ€ì„ì•„ì›ƒ ë°œìƒ!")
            print(f"[INFO] ì„œë²„ ì•ˆì •í™”ë¥¼ ìœ„í•´ {wait_minutes:.1f}ë¶„ ëŒ€ê¸°í•©ë‹ˆë‹¤...")

            remaining_time = wait_time
            while remaining_time > 0:
                minutes_left = remaining_time / 60
                print(f"[INFO] ë‚¨ì€ ëŒ€ê¸° ì‹œê°„: {minutes_left:.1f}ë¶„")

                sleep_duration = min(30, remaining_time)
                time.sleep(sleep_duration)
                remaining_time -= sleep_duration

            print(f"[INFO] ëŒ€ê¸° ì™„ë£Œ! í¬ë¡¤ë§ì„ ì¬ê°œí•©ë‹ˆë‹¤.")
            self.consecutive_timeouts = 0

    def start(self) -> None:
        """v1.7: ë‹¤ì¤‘ ìƒí’ˆ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë©”ì¸ ì‹œì‘ í•¨ìˆ˜ (JSON ì§€ì›)"""
        print("=" * 70)
        print("ğŸ›’ ì¿ íŒ¡ ë¦¬ë·° í¬ë¡¤ëŸ¬ v1.7 (JSON ì§€ì› + ëœë¤ í”„ë¡ì‹œ)")
        print("=" * 70)

        # JSON íŒŒì¼ ë¡œë“œ
        if not self.url_manager.load_urls_from_json():
            print("[ERROR] JSON íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        total_products = len(self.url_manager.products)
        print(f"[INFO] ì´ {total_products}ê°œ ìƒí’ˆì„ ìˆœì°¨ì ìœ¼ë¡œ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")
        print(f"[INFO] ê° ìƒí’ˆë‹¹ ìµœëŒ€ {self.max_pages}í˜ì´ì§€ê¹Œì§€ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")
        print(f"[INFO] ì—°ì† 5ë²ˆ ë¦¬ë·° ì—†ìŒ ê°ì§€ì‹œ ë‹¤ìŒ ìƒí’ˆìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")

        # í”„ë¡ì‹œ ì‚¬ìš© ì •ë³´ ì¶œë ¥
        if self.proxy_rotator and self.proxy_rotator.proxy_list:
            available_proxies = self.proxy_rotator.get_available_proxy_count()
            print(f"[INFO] ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œ: {available_proxies}/{len(self.proxy_rotator.proxy_list)}ê°œ")
            print(f"[INFO] ğŸ² í”„ë¡ì‹œ ëœë¤ ì„ íƒ ëª¨ë“œ í™œì„±í™”")

        print("=" * 70)

        # ì „ì²´ í†µê³„
        total_success_products = 0
        total_failed_products = 0
        overall_start_time = time.time()

        # ìƒí’ˆë³„ í¬ë¡¤ë§ ì‹¤í–‰
        while True:
            product = self.url_manager.get_next_product()
            if not product:
                break

            current_progress, total_progress = self.url_manager.get_current_progress()
            print(f"\n{'=' * 20} ìƒí’ˆ {current_progress}/{total_progress} {'=' * 20}")
            print(f"[INFO] í˜„ì¬ ìƒí’ˆ: {product['name']}")
            print(f"[INFO] ìƒí’ˆ URL: {product['url']}")

            try:
                success = self.crawl_single_product(product['url'], product['name'])
                if success:
                    total_success_products += 1
                    print(f"âœ… ìƒí’ˆ {current_progress} í¬ë¡¤ë§ ì„±ê³µ")
                else:
                    total_failed_products += 1
                    print(f"âŒ ìƒí’ˆ {current_progress} í¬ë¡¤ë§ ì‹¤íŒ¨")

            except KeyboardInterrupt:
                print(f"\n[INFO] ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
                print(f"[INFO] ì§„í–‰ë¥ : {current_progress - 1}/{total_progress} ì™„ë£Œ")
                break
            except Exception as e:
                print(f"[ERROR] ìƒí’ˆ í¬ë¡¤ë§ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                total_failed_products += 1
                continue

            # ìƒí’ˆ ê°„ ëŒ€ê¸° ì‹œê°„
            if self.url_manager.get_remaining_count() > 0:
                delay = random.uniform(10, 20)  # ìƒí’ˆ ê°„ 10-20ì´ˆ ëŒ€ê¸°
                print(f"[INFO] ë‹¤ìŒ ìƒí’ˆê¹Œì§€ {delay:.1f}ì´ˆ ëŒ€ê¸°...")
                time.sleep(delay)

        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        overall_end_time = time.time()
        total_elapsed = overall_end_time - overall_start_time

        print("\n" + "=" * 70)
        print("ğŸ“Š ì „ì²´ í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
        print("=" * 70)
        print(f"ì´ ìƒí’ˆ ìˆ˜: {total_products}ê°œ")
        print(f"ì„±ê³µí•œ ìƒí’ˆ: {total_success_products}ê°œ")
        print(f"ì‹¤íŒ¨í•œ ìƒí’ˆ: {total_failed_products}ê°œ")
        print(f"ì„±ê³µë¥ : {(total_success_products / total_products * 100):.1f}%")
        print(f"ì´ ì†Œìš” ì‹œê°„: {total_elapsed / 60:.1f}ë¶„")
        print(f"ğŸ“ ê²°ê³¼ íŒŒì¼ë“¤ì€ 'Coupang-reviews' í´ë”ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")
        print("=" * 70)

    def crawl_single_product(self, url: str, product_name: str) -> bool:
        """ë‹¨ì¼ ìƒí’ˆ í¬ë¡¤ë§"""
        if '#' in url:
            url = url.split('#')[0]
            print(f"[DEBUG] URL fragment ì œê±°: {url}")

        prod_code: str = self.get_product_code(url=url)
        print(f"[DEBUG] ìƒí’ˆ ì½”ë“œ: {prod_code}")

        # ì„¸ì…˜ ì˜ˆì—´
        self.warm_up_session(prod_code)

        # ìƒí’ˆë³„ SaveData ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        sd = SaveData()

        try:
            self.title = self.get_product_title(product_name=product_name)
            print(f"[INFO] ìƒí’ˆëª…: {self.title}")
        except Exception as e:
            print(f"[ERROR] ìƒí’ˆëª…ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë„ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            self.title = "ìƒí’ˆëª… ë¯¸í™•ì¸"

        self.page_title = None  # í˜ì´ì§€ íƒ€ì´í‹€ ì´ˆê¸°í™”
        success_count = 0
        current_page = 1
        consecutive_empty_pages = 0
        max_empty_pages = 5  # v1.6: ì—°ì† ë¹ˆ í˜ì´ì§€ í—ˆìš© íšŸìˆ˜ (5ë²ˆ ì—°ì† ë¦¬ë·° ì—†ìŒì‹œ ë‹¤ìŒ ìƒí’ˆìœ¼ë¡œ)
        proxy_change_attempts = 0

        product_start_time = time.time()

        while consecutive_empty_pages < max_empty_pages and current_page <= self.max_pages:
            payload = {
                "productId": prod_code,
                "page": current_page,
                "size": 10,
                "sortBy": "DATE_DESC",
                "ratings": "",
                "q": "",
                "viRoleCode": 2,
                "ratingSummary": False,
            }

            result = self.fetch(payload=payload, sd=sd)

            if result:
                success_count += 1
                consecutive_empty_pages = 0
                proxy_change_attempts = 0
            else:
                consecutive_empty_pages += 1
                print(f"[WARNING] í˜ì´ì§€ {current_page}ì—ì„œ ë¦¬ë·°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ({consecutive_empty_pages}/{max_empty_pages})")

                # ì—°ì† ë¹ˆ í˜ì´ì§€ê°€ 2ê°œ ì´ìƒì´ê³  í”„ë¡ì‹œë¥¼ ì‚¬ìš© ì¤‘ì´ë¼ë©´ í”„ë¡ì‹œ ìƒíƒœ ì²´í¬
                if (consecutive_empty_pages >= 2 and
                        self.proxy_rotator and
                        self.proxy_rotator.current_proxy and
                        proxy_change_attempts < 3):

                    available_proxies = self.proxy_rotator.get_available_proxy_count()
                    if available_proxies > 1:
                        print(f"[INFO] ì—°ì† ì‹¤íŒ¨ë¡œ ì¸í•œ í”„ë¡ì‹œ êµì²´ ì‹œë„ ({proxy_change_attempts + 1}/3)")
                        self.proxy_rotator.mark_proxy_failed(self.proxy_rotator.current_proxy)
                        proxy_change_attempts += 1
                        print(f"[INFO] í˜ì´ì§€ {current_page} ë‹¤ë¥¸ í”„ë¡ì‹œë¡œ ì¬ì‹œë„...")
                        continue

            current_page += 1

            if result and consecutive_empty_pages == 0:
                short_delay = random.uniform(1.0, 3.0)
                time.sleep(short_delay)

        product_end_time = time.time()
        product_elapsed = product_end_time - product_start_time

        # ìƒí’ˆë³„ ê²°ê³¼ ì¶œë ¥
        print(f"\n[PRODUCT SUMMARY] ìƒí’ˆ '{self.title}' í¬ë¡¤ë§ ì™„ë£Œ")
        print(f"[INFO] ì„±ê³µ í˜ì´ì§€: {success_count}ê°œ (ì´ {current_page - 1}í˜ì´ì§€ ì‹œë„)")
        print(f"[INFO] ì†Œìš” ì‹œê°„: {product_elapsed / 60:.1f}ë¶„")

        if consecutive_empty_pages >= max_empty_pages:
            print(f"[INFO] ì—°ì† {max_empty_pages}ë²ˆ ë¹ˆ í˜ì´ì§€ë¡œ ì¸í•´ ë‹¤ìŒ ìƒí’ˆìœ¼ë¡œ ì§„í–‰")
        elif current_page > self.max_pages:
            print(f"[INFO] ìµœëŒ€ í˜ì´ì§€ ìˆ˜({self.max_pages})ì— ë„ë‹¬í•˜ì—¬ ì™„ë£Œ")

        return success_count > 0

    def fetch(self, payload: dict, sd) -> bool:
        now_page: int = payload["page"]
        print(f"\n[INFO] Start crawling page {now_page} ...")
        attempt: int = 0
        proxy_attempts: int = 0
        max_proxy_attempts: int = min(10, len(self.proxy_rotator.proxy_list) if self.proxy_rotator else 0)

        while attempt < self.retries:
            try:
                # ë§¤ ìš”ì²­ë§ˆë‹¤ ìƒˆë¡œìš´ User-Agent ì‚¬ìš©
                if attempt > 0:
                    self.update_headers()

                session = self.get_session_with_proxy()
                session.cookies.update(self.session.cookies)
                session.headers.update({
                    "Referer": f"https://www.coupang.com/vp/products/{payload['productId']}"
                })

                resp = session.get(
                    url=self.base_review_url,
                    params=payload,
                    timeout=(15, 30),
                )

                self.consecutive_timeouts = 0

                if resp.status_code == 403:
                    print(f"[ERROR] HTTP 403 ì‘ë‹µ - í”„ë¡ì‹œê°€ ì°¨ë‹¨ë¨")
                    if self.proxy_rotator and self.proxy_rotator.current_proxy:
                        self.proxy_rotator.mark_proxy_failed(self.proxy_rotator.current_proxy)
                    attempt += 1
                    continue
                elif resp.status_code != 200:
                    print(f"[ERROR] HTTP {resp.status_code} ì‘ë‹µ")
                    attempt += 1
                    continue

                html = resp.text
                soup = bs(html, "html.parser")

                if self.page_title is None:
                    first_review = soup.select_one("article.sdp-review__article__list")
                    if first_review:
                        title_elem = first_review.select_one("div.sdp-review__article__list__info__product-info__name")
                        self.page_title = title_elem.text.strip() if title_elem else self.title
                    else:
                        self.page_title = self.title

                articles = soup.select("article.sdp-review__article__list")
                article_length = len(articles)

                if article_length == 0:
                    print(f"[WARNING] í˜ì´ì§€ {now_page}ì—ì„œ ë¦¬ë·°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                    # í”„ë¡ì‹œ ì‚¬ìš© ì¤‘ì´ë¼ë©´ ë‹¤ë¥¸ í”„ë¡ì‹œë¡œ ì¬ì‹œë„
                    if self.proxy_rotator and self.proxy_rotator.current_proxy and proxy_attempts < max_proxy_attempts:
                        print(f"[INFO] í”„ë¡ì‹œ ì°¨ë‹¨ ê°€ëŠ¥ì„±ìœ¼ë¡œ ë‹¤ë¥¸ í”„ë¡ì‹œë¡œ ì¬ì‹œë„ ({proxy_attempts + 1}/{max_proxy_attempts})")
                        self.proxy_rotator.mark_proxy_failed(self.proxy_rotator.current_proxy)
                        proxy_attempts += 1
                        attempt += 1
                        retry_delay = random.uniform(1.0, 3.0)
                        print(f"[DEBUG] {retry_delay:.1f}ì´ˆ í›„ ë‹¤ë¥¸ í”„ë¡ì‹œë¡œ ì¬ì‹œë„...")
                        time.sleep(retry_delay)
                        continue

                    # ì°¨ë‹¨ ê°ì§€ ë° ì¶”ê°€ ì²˜ë¦¬
                    if now_page == 1:
                        print("[DEBUG] ì²« í˜ì´ì§€ HTML êµ¬ì¡° í™•ì¸:")
                        print(f"  - ì „ì²´ ê¸¸ì´: {len(html)} ë¬¸ì")
                        print(f"  - 'review' í¬í•¨ íšŸìˆ˜: {html.lower().count('review')}")
                        print(f"  - 'article' í¬í•¨ íšŸìˆ˜: {html.lower().count('article')}")

                        blocked_indicators = [
                            "access denied", "blocked", "forbidden",
                            "captcha", "robot", "bot", "security", "verification"
                        ]

                        html_lower = html.lower()
                        is_blocked = False
                        for indicator in blocked_indicators:
                            if indicator in html_lower:
                                print(f"[WARNING] ì°¨ë‹¨ ê°ì§€: '{indicator}' ë°œê²¬")
                                is_blocked = True
                                break

                        if is_blocked and attempt < self.retries - 2:
                            print("[INFO] ì°¨ë‹¨ ê°ì§€ë¡œ ì¸í•œ ì¶”ê°€ ì¬ì‹œë„...")
                            attempt += 1
                            long_delay = random.uniform(5.0, 10.0)
                            print(f"[DEBUG] {long_delay:.1f}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                            time.sleep(long_delay)
                            continue

                    return False

                print(f"[SUCCESS] í˜ì´ì§€ {now_page}ì—ì„œ {article_length}ê°œ ë¦¬ë·° ë°œê²¬")

                # ë¦¬ë·° ë°ì´í„° ì²˜ë¦¬
                for idx in range(article_length):
                    dict_data: dict[str, str | int] = dict()

                    review_date_elem = articles[idx].select_one(
                        "div.sdp-review__article__list__info__product-info__reg-date"
                    )
                    review_date = review_date_elem.text.strip() if review_date_elem else "-"

                    user_name_elem = articles[idx].select_one(
                        "span.sdp-review__article__list__info__user__name"
                    )
                    user_name = user_name_elem.text.strip() if user_name_elem else "-"

                    rating_elem = articles[idx].select_one(
                        "div.sdp-review__article__list__info__product-info__star-orange"
                    )
                    if rating_elem and rating_elem.get("data-rating"):
                        try:
                            rating = int(rating_elem.get("data-rating"))
                        except (ValueError, TypeError):
                            rating = 0
                    else:
                        rating = 0

                    prod_name_elem = articles[idx].select_one(
                        "div.sdp-review__article__list__info__product-info__name"
                    )
                    prod_name = prod_name_elem.text.strip() if prod_name_elem else "-"

                    headline_elem = articles[idx].select_one(
                        "div.sdp-review__article__list__headline"
                    )
                    headline = headline_elem.text.strip() if headline_elem else ""

                    review_content_elem = articles[idx].select_one(
                        "div.sdp-review__article__list__review__content.js_reviewArticleContent"
                    )
                    if review_content_elem:
                        review_content = re.sub("[\n\t]", "", review_content_elem.text.strip())
                    else:
                        review_content_elem = articles[idx].select_one(
                            "div.sdp-review__article__list__review > div"
                        )
                        if review_content_elem:
                            review_content = re.sub("[\n\t]", "", review_content_elem.text.strip())
                        else:
                            review_content = ""

                    helpful_count_elem = articles[idx].select_one("span.js_reviewArticleHelpfulCount")
                    helpful_count = helpful_count_elem.text.strip() if helpful_count_elem else "0"

                    review_images = articles[idx].select("div.sdp-review__article__list__attachment__list img")
                    image_count = len(review_images)

                    dict_data["title"] = self.page_title
                    dict_data["prod_name"] = prod_name
                    dict_data["review_date"] = review_date
                    dict_data["user_name"] = user_name
                    dict_data["rating"] = rating
                    dict_data["headline"] = headline
                    dict_data["review_content"] = review_content
                    dict_data["helpful_count"] = helpful_count
                    dict_data["image_count"] = image_count

                    sd.save(datas=dict_data)
                    print(f"[SUCCESS] ë¦¬ë·° ì €ì¥ ì™„ë£Œ: {user_name} - {rating}ì ")

                page_delay = random.uniform(self.page_delay_min, self.page_delay_max)
                print(f"[DEBUG] ë‹¤ìŒ í˜ì´ì§€ê¹Œì§€ {page_delay:.1f}ì´ˆ ëŒ€ê¸°...")
                time.sleep(page_delay)
                return True

            except RequestException as e:
                attempt += 1

                error_str = str(e).lower()
                is_proxy_error = any(keyword in error_str for keyword in [
                    "403", "proxy", "connection", "timeout", "refused", "unreachable"
                ])

                if is_proxy_error and self.proxy_rotator and self.proxy_rotator.current_proxy:
                    self.proxy_rotator.mark_proxy_failed(self.proxy_rotator.current_proxy)
                    print("[INFO] í”„ë¡ì‹œ ì˜¤ë¥˜ë¡œ ì¸í•œ ë‹¤ë¥¸ í”„ë¡ì‹œë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤.")

                    available_proxies = self.proxy_rotator.get_available_proxy_count()
                    if available_proxies > 0:
                        print(f"[INFO] ë‚¨ì€ ì‚¬ìš© ê°€ëŠ¥ í”„ë¡ì‹œ: {available_proxies}ê°œ")
                    else:
                        print("[WARNING] ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")

                if self.is_timeout_error(e):
                    self.consecutive_timeouts += 1
                    print(f"[ERROR] íƒ€ì„ì•„ì›ƒ ë°œìƒ (ì—°ì† {self.consecutive_timeouts}íšŒ): {e}")

                    if self.consecutive_timeouts >= self.max_consecutive_timeouts:
                        self.handle_consecutive_timeouts()
                else:
                    self.consecutive_timeouts = 0
                    print(f"[ERROR] ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")

                print(f"[ERROR] Attempt {attempt}/{self.retries} failed")
                if attempt < self.retries:
                    retry_delay = random.uniform(self.delay_min, self.delay_max)
                    print(f"[DEBUG] {retry_delay:.1f}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(retry_delay)
                else:
                    print(f"[ERROR] ìµœëŒ€ ìš”ì²­ íšŸìˆ˜ ì´ˆê³¼! í˜ì´ì§€ {now_page} í¬ë¡¤ë§ ì‹¤íŒ¨.")
                    return False
            except Exception as e:
                print(f"[ERROR] ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
                self.consecutive_timeouts = 0
                return False

        return False

    @staticmethod
    def clear_console() -> None:
        command: str = "clear"
        if os.name in ("nt", "dos"):
            command = "cls"
        try:
            if os.environ.get('TERM') is None:
                os.environ['TERM'] = 'xterm'
            os.system(command=command)
        except:
            pass


class SaveData:
    def __init__(self) -> None:
        self.wb: Workbook = Workbook()
        self.ws = self.wb.active
        self.ws.append([
            "ìƒí’ˆëª…", "êµ¬ë§¤ìƒí’ˆëª…", "ì‘ì„±ì¼ì", "êµ¬ë§¤ìëª…", "í‰ì ",
            "í—¤ë“œë¼ì¸", "ë¦¬ë·°ë‚´ìš©", "ë„ì›€ìˆ˜", "ì´ë¯¸ì§€ìˆ˜"
        ])
        self.row: int = 2
        self.dir_name: str = "data/Coupang-reviews-homeplanet"
        self.create_directory()

    def create_directory(self) -> None:
        if not os.path.exists(self.dir_name):
            os.makedirs(self.dir_name)
            print(f"[INFO] ë””ë ‰í† ë¦¬ ìƒì„±: {self.dir_name}")

    def save(self, datas: dict[str, str | int]) -> None:
        try:
            safe_title = re.sub(r'[<>:"/\\|?*]', '_', datas["title"])
            file_name: str = os.path.join(self.dir_name, safe_title + ".xlsx")

            self.ws[f"A{self.row}"] = datas["title"]
            self.ws[f"B{self.row}"] = datas["prod_name"]
            self.ws[f"C{self.row}"] = datas["review_date"]
            self.ws[f"D{self.row}"] = datas["user_name"]
            self.ws[f"E{self.row}"] = datas["rating"]
            self.ws[f"F{self.row}"] = datas["headline"]
            self.ws[f"G{self.row}"] = datas["review_content"]
            self.ws[f"H{self.row}"] = datas["helpful_count"]
            self.ws[f"I{self.row}"] = datas["image_count"]

            self.row += 1
            self.wb.save(filename=file_name)

        except Exception as e:
            print(f"[ERROR] ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def __del__(self) -> None:
        try:
            if hasattr(self, 'wb'):
                self.wb.close()
        except:
            pass


def load_proxy_list_from_file(file_path="env/proxy_list.txt"):
    """txt íŒŒì¼ì—ì„œ í”„ë¡ì‹œ ëª©ë¡ ë¡œë“œ"""
    try:
        if not os.path.exists(file_path):
            create_sample_proxy_file(file_path)
            return []

        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        proxy_list = []

        for line_num, line in enumerate(lines, 1):
            line = line.strip()

            # ë¹ˆ ì¤„ì´ë‚˜ ì£¼ì„(#ìœ¼ë¡œ ì‹œì‘) ê±´ë„ˆë›°ê¸°
            if not line or line.startswith('#'):
                continue

            # í”„ë¡ì‹œ í˜•ì‹ ê²€ì¦ (ip:port:username:password)
            parts = line.split(':')
            if len(parts) == 4:
                ip, port, username, password = parts
                # ê¸°ë³¸ì ì¸ IPì™€ í¬íŠ¸ ê²€ì¦
                if is_valid_proxy_format(ip, port):
                    proxy_list.append(line)
                    print(f"[INFO] í”„ë¡ì‹œ ë¡œë“œ: {ip}:{port}")
                else:
                    print(f"[WARNING] ì˜ëª»ëœ í”„ë¡ì‹œ í˜•ì‹ (ë¼ì¸ {line_num}): {line}")
            else:
                print(f"[WARNING] ì˜ëª»ëœ í˜•ì‹ (ë¼ì¸ {line_num}): {line}")
                print("         ì˜¬ë°”ë¥¸ í˜•ì‹: ip:port:username:password")

        if proxy_list:
            print(f"[SUCCESS] {len(proxy_list)}ê°œì˜ ìœ íš¨í•œ í”„ë¡ì‹œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            return proxy_list
        else:
            print("[ERROR] ìœ íš¨í•œ í”„ë¡ì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

    except Exception as e:
        print(f"[ERROR] í”„ë¡ì‹œ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        return []


def create_sample_proxy_file(file_path="proxy_list.txt"):
    """ìƒ˜í”Œ í”„ë¡ì‹œ íŒŒì¼ ìƒì„±"""
    sample_content = """# í”„ë¡ì‹œ ëª©ë¡ íŒŒì¼
# í˜•ì‹: ip:port:username:password
# í•œ ì¤„ì— í•˜ë‚˜ì”© ì…ë ¥í•˜ì„¸ìš”
# '#'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ì€ ì£¼ì„ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤

# ìƒ˜í”Œ í”„ë¡ì‹œ (ì‹¤ì œ í”„ë¡ì‹œë¡œ êµì²´í•˜ì„¸ìš”)
173.214.177.18:5709:daxvymvx:kn518nmfd34a
198.23.214.119:6386:daxvymvx:kn518nmfd34a
50.114.98.49:5533:daxvymvx:kn518nmfd34a

# ì¶”ê°€ í”„ë¡ì‹œë“¤ì„ ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”
# 192.168.1.1:8080:user:pass
# 10.0.0.1:3128:admin:password
"""

    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(sample_content)
        print(f"[INFO] ìƒ˜í”Œ í”„ë¡ì‹œ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {file_path}")
        print("[INFO] íŒŒì¼ì„ í¸ì§‘í•˜ì—¬ ì‹¤ì œ í”„ë¡ì‹œ ì •ë³´ë¥¼ ì…ë ¥í•œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
    except Exception as e:
        print(f"[ERROR] ìƒ˜í”Œ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")


def is_valid_proxy_format(ip, port):
    """ê¸°ë³¸ì ì¸ IPì™€ í¬íŠ¸ í˜•ì‹ ê²€ì¦"""
    try:
        # IP ì£¼ì†Œ í˜•ì‹ ê²€ì¦ (ê°„ë‹¨í•œ ê²€ì¦)
        ip_parts = ip.split('.')
        if len(ip_parts) != 4:
            return False

        for part in ip_parts:
            if not part.isdigit() or not (0 <= int(part) <= 255):
                return False

        # í¬íŠ¸ ë²ˆí˜¸ ê²€ì¦
        if not port.isdigit() or not (1 <= int(port) <= 65535):
            return False

        return True
    except:
        return False


def test_proxy(proxy_string):
    """í”„ë¡ì‹œ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        parts = proxy_string.split(':')
        if len(parts) == 4:
            ip, port, username, password = parts
            proxy_dict = {
                'http': f'http://{username}:{password}@{ip}:{port}',
                'https': f'http://{username}:{password}@{ip}:{port}'
            }
        else:
            return False

        response = rq.get('http://httpbin.org/ip', proxies=proxy_dict, timeout=10)
        if response.status_code == 200:
            return True
    except:
        pass
    return False


def get_proxy_list():
    """í”„ë¡ì‹œ ëª©ë¡ ë°˜í™˜ (íŒŒì¼ì—ì„œ ë¡œë“œ)"""
    proxy_file_path = "env/proxy_list.txt"

    print("=" * 70)
    print("ğŸ”— í”„ë¡ì‹œ ì„¤ì •")
    print("=" * 70)

    # í”„ë¡ì‹œ íŒŒì¼ì—ì„œ ë¡œë“œ
    proxy_list = load_proxy_list_from_file(proxy_file_path)

    if not proxy_list:
        print(f"\n[WARNING] {proxy_file_path} íŒŒì¼ì— ìœ íš¨í•œ í”„ë¡ì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")

        # í”„ë¡ì‹œ ì—†ì´ ì‹¤í–‰í• ì§€ í™•ì¸
        run_without_proxy = input("í”„ë¡ì‹œ ì—†ì´ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n): ").lower().strip()
        if run_without_proxy != 'n':
            print("[INFO] í”„ë¡ì‹œ ì—†ì´ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            return None
        else:
            print("[INFO] í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            print(f"[INFO] {proxy_file_path} íŒŒì¼ì„ í¸ì§‘í•˜ì—¬ í”„ë¡ì‹œë¥¼ ì¶”ê°€í•œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
            exit(0)

    print(f"\n[INFO] {len(proxy_list)}ê°œì˜ í”„ë¡ì‹œê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print()

    # í”„ë¡ì‹œ ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    use_proxy = input("í”„ë¡ì‹œë¥¼ ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n): ").lower().strip()

    if use_proxy == 'n':
        print("[INFO] í”„ë¡ì‹œ ì—†ì´ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        return None
    else:
        print("[INFO] í”„ë¡ì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤.")

        # í”„ë¡ì‹œ í…ŒìŠ¤íŠ¸ ì—¬ë¶€ í™•ì¸
        test_proxies = input("í”„ë¡ì‹œ ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower().strip()

        if test_proxies == 'y':
            print("\n[INFO] í”„ë¡ì‹œ ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
            working_proxies = []

            for i, proxy in enumerate(proxy_list, 1):
                print(f"[TEST] {i}/{len(proxy_list)} - {proxy.split(':')[0]}:{proxy.split(':')[1]} í…ŒìŠ¤íŠ¸ ì¤‘...", end='')
                if test_proxy(proxy):
                    print(" ì„±ê³µ")
                    working_proxies.append(proxy)
                else:
                    print(" ì‹¤íŒ¨")

            if working_proxies:
                print(f"\n[SUCCESS] {len(working_proxies)}/{len(proxy_list)}ê°œ í”„ë¡ì‹œê°€ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
                print(f"[INFO] ì‘ë™í•˜ëŠ” í”„ë¡ì‹œë§Œ ì‚¬ìš©í•˜ì—¬ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                return working_proxies
            else:
                print("\n[ERROR] ì‘ë™í•˜ëŠ” í”„ë¡ì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")
                fallback = input("í”„ë¡ì‹œ ì—†ì´ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n): ").lower().strip()
                if fallback != 'n':
                    print("[INFO] í”„ë¡ì‹œ ì—†ì´ ì‹¤í–‰í•©ë‹ˆë‹¤.")
                    return None
                else:
                    print("[INFO] í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    exit(0)
        else:
            print("[INFO] í…ŒìŠ¤íŠ¸ ì—†ì´ ëª¨ë“  í”„ë¡ì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            print("[INFO] ì‹¤í–‰ ì¤‘ ìë™ìœ¼ë¡œ ì‘ë™í•˜ì§€ ì•ŠëŠ” í”„ë¡ì‹œë¥¼ ì œì™¸í•©ë‹ˆë‹¤.")
            return proxy_list


if __name__ == "__main__":
    try:
        # í”„ë¡ì‹œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        proxy_list = get_proxy_list()

        # í¬ë¡¤ëŸ¬ ì‹œì‘
        coupang = Coupang(proxy_list=proxy_list)
        coupang.start()

        print("\n" + "=" * 70)
        print("ëª¨ë“  ìƒí’ˆ í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ê²°ê³¼ íŒŒì¼ë“¤ì€ 'Coupang-reviews' í´ë”ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")
        print("=" * 70)

    except KeyboardInterrupt:
        print("\n[INFO] ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\n[ERROR] í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()
    finally:
        print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")